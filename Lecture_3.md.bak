# 第3节: 编码与最优码性质

## 〇、引言

本节课程的核心是探讨在给定信源符号及其概率分布后，如何设计一种最优的二进制编码方案，以实现最高效的数据表示，即最小化平均码长。我们将回顾熵作为这一目标的理论下限，并深入研究一个“最优码”应该具备哪些固有性质。这些性质不仅加深了我们对编码理论的理解，也为后续学习具体的编码算法（如哈夫曼编码）奠定了坚实的理论基础。

-   **背景故事引入：** 教师提及哈夫曼 (Huffman) 在学生时代，因面临一门课程可能不及格的压力，从而构思出了著名的哈夫曼编码。这个故事启发我们，许多原创性的重要工作往往在学术生涯的早期阶段完成。
-   **问题设定：** 为了更好地理解最优编码的设计思路，我们假设回到哈夫曼编码尚未被发明的时代（约1951-1952年），从头开始思考如何解决编码问题。

-   **回顾熵与平均码长下界 (源自 Lecture_3_Part_1)：**
    在讨论实际编码设计之前，我们曾推导过，对于一个离散随机变量 $X$，其概率质量函数 (PMF) 为 $P=(p_1, \dots, p_n)$，若允许码长 $L_i$ 为实数，并且编码是前缀无关的（满足 Kraft 不等式 $\sum 2^{-L_i} \le 1$），则最小化平均码长 $\sum p_i L_i$ 的理论下限是信源的**熵 (Entropy)** $H(X)$：
    $$H(X) = \sum_{i=1}^{n} p_i \log_2 \frac{1}{p_i} = -\sum_{i=1}^{n} p_i \log_2 p_i \quad (\text{单位: 比特})$$
    这个结论是通过以下优化问题得到的：
    $$ \min_{L_1, ..., L_n} \sum_{i=1}^{n} p_i L_i $$
    s.t.
    - $\sum_{i=1}^{n} 2^{-L_i} \le 1$ (后被强化为 $=1$)
    - $L_i > 0$ (理论分析时放宽为实数)

    香农指出，实际编码（码长为整数）的平均码长 $L_{avg}$ 满足 $H(X) \le L_{avg} < H(X) + 1$。我们的目标就是设计出能尽可能逼近这个下界的实际编码方案。

---

## 一、编码问题的数学描述

### 1. 给定 (Input):

-   一个离散随机变量 $X$ (Notation: r.v. X)。
-   其概率质量函数 (PMF) 为 $P = (p_1, p_2, \dots, p_n)$，其中 $p_i$ 是 $X$ 取第 $i$ 个符号 $x_i$ 的概率, $p_i = P(X=x_i)$。

> *(板书 (0:52 - 1:02, Part_2/3):*
>
> **r.v. X** $P=(p_1, \dots, p_n)$

### 2. 目标 (Goal):

-   找到一个编码方案，即一组二进制**码字 (Codewords)** $C = (c_1, c_2, \dots, c_n)$。

> *(板书 (1:11 - 1:19, Part_2/3):*
>
> **Code** $C=(c_1, \dots, c_n)$

-   这是一个**二进制编码 (Binary Code)**，意味着每个码字 $c_i$ 都是由0和1组成的有限长度字符串，即 $c_i \in \{0,1\}^*$ (其中 $\{0,1\}^*$ 表示所有可能的有限长度二进制串的集合)。

> *(板书 (1:28 - 1:35, Part_3 或 2:48 - 2:53, Part_2):*
>
> $c_i \in \{0,1\}^*$

### 3. 优化目标：最小化平均码长 (Minimize Average Code Length)

-   设 $l(c_i)$ 为码字 $c_i$ 的长度（即比特数）。平均码长 $\bar{L}$ 定义为：
    $$ \bar{L} = \sum_{i=1}^{n} p_i l(c_i) $$
-   我们的目标是使 $\bar{L}$ 最小。

> *(板书 (1:40 - 1:57, Part_2/3):*
>
> **Goal:** $\sum P_i l(c_i)$ is minimum.

> *(注：教师在板书中常使用 $|C_i|$ 来代表码字 $c_i$ 的长度 $l(c_i)$。)*

### 4. 最优码 (Optimal Code):

-   使平均码长 $\bar{L}$ 最小的编码方案称为**最优码**。

> *(板书 (2:25, Part_3):*
>
> `optimal code`

### 5. 重要约束 (Constraints):

-   **整数码长：** 在实际编码中，码字长度 $l(c_i)$ **必须是正整数**。这与理论推导熵时允许码长为实数不同。
-   **前缀无关码 (Prefix-free code)：** 编码必须是前缀无关的，以保证**唯一可译性 (Uniquely Decodable)** 和**即时解码 (Instantaneous Decodability)**。这意味着码集中任何一个码字都不能是另一个码字的前缀。虽然唯一可译码的范围更广，但可以证明，对于最小化平均码长的目标，最优码总可以在前缀无关码中找到（或转换为一个具有相同平均码长的前缀无关码）。

---

## 二、最优码的性质 (Properties of Optimal Code)

在着手设计具体的编码算法（如哈夫曼编码）之前，我们先分析一个“最优码”应该具备哪些结构上的性质。这些性质将为算法设计提供关键的指导。

-   **思考起点：** 假设我们已经有了一个最优码，它应该长什么样子？

### 1. 性质1：概率与码长的单调非增关系 (Monotonicity)

-   **假设：** 将信源符号按其概率降序排列，即 $p_1 \ge p_2 \ge \dots \ge p_n$。
-   **结论：** 那么，在任何一个最优码中，对应的码字长度必然满足（非严格）升序排列：
    $l(c_1) \le l(c_2) \le \dots \le l(c_n)$。
-   **解释 (基于反证法)：**
    直观上，为了使加权平均码长 $\sum p_i l(c_i)$ 最小，概率越大的符号，其码长应该越短（或至少不比概率更小的符号的码长更长）。
    严格来说，假设存在某个最优码，其中有两个符号 $x_j$ 和 $x_k$ 满足 $p_j > p_k$ 但其码长 $l(c_j) > l(c_k)$。我们可以交换这两个符号的码字分配，得到一个新的编码方案 $C'$。
    原平均码长 $\bar{L} = \dots + p_j l(c_j) + p_k l(c_k) + \dots$
    新平均码长 $\bar{L}' = \dots + p_j l(c_k) + p_k l(c_j) + \dots$
    两者之差为 $\bar{L}' - \bar{L} = p_j l(c_k) + p_k l(c_j) - p_j l(c_j) - p_k l(c_k) = (p_j - p_k)l(c_k) - (p_j - p_k)l(c_j) = (p_j - p_k)(l(c_k) - l(c_j))$。
    由于 $p_j > p_k$ (即 $p_j - p_k > 0$) 且假设 $l(c_j) > l(c_k)$ (即 $l(c_k) - l(c_j) < 0$)，所以 $(p_j - p_k)(l(c_k) - l(c_j)) < 0$。这意味着 $\bar{L}' < \bar{L}$。
    这就构造出了一个平均码长更短的新编码，与原编码是最优的假设相矛盾。因此，假设不成立。
    如果 $p_j = p_k$，则交换码字不改变平均码长，所以允许 $l(c_j)$ 和 $l(c_k)$ 之间任意关系，但为了满足非严格单调性，仍可要求 $l(c_j) \le l(c_k)$ 或 $l(c_k) \le l(c_j)$。通常我们按概率排序后，码长也对应排序。

> *(板书 (5:38, Part_2/3):*
>
> **Properties of Optimal Code**
>
> **1. Assume** $p_1 \ge p_2 \ge \dots \ge p_n \implies$ **optimal code must satisfy** $|c_1| \le |c_2| \le \dots \le |c_n|$

> *(这里的 $|c_i|$ 即 $l(c_i)$)*

### 2. 性质2：最优前缀码的Kraft不等式取等 (Kraft Inequality holds with equality for optimal prefix-free codes)

-   **结论：** 对于一个最优的前缀无关码，其码字长度 $l(c_i)$ 必须使 Kraft 不等式取等号：
    $$ \sum_{i=1}^{n} 2^{-l(c_i)} = 1 $$
-   **解释 (基于二叉树表示)：**
    -   任何前缀无关码都可以用一个二叉树来表示，其中每个码字对应树的一个叶节点。码字的长度 $l(c_i)$ 等于该叶节点到根节点的深度。
    -   Kraft 不等式 $\sum_{i=1}^{n} 2^{-l(c_i)} \le 1$ 表明所有叶节点“占据”的“码空间份额”之和不能超过1。
    -   如果对于一个前缀无关码，$\sum 2^{-l(c_i)} < 1$，这意味着其对应的二叉树不是“满的”(full)。也就是说，树中存在某个内部节点只有一个子节点，或者存在某个叶节点（码字）可以被其父节点替代（如果父节点没有另一个子节点）从而缩短路径，或者存在一个深度小于最长码字长度的内部节点，它没有子节点，本可以生长出新的子节点。
    -   在这种“不满”的情况下，总可以通过调整编码树的结构来得到一个新的前缀无关码，其平均码长更短。例如：
        -   如果一个内部节点只有一个子节点，可以将这个子节点上提一级，替换掉其父节点，这样所有以该子节点为根的子树中的码字长度都会减1，从而使平均码长减小。
        -   如果存在一个码字 $c_k$ 使得它可以被缩短（例如，它的父节点 $u$ 只有 $c_k$ 这一个叶子节点作为孩子），我们可以用 $u$ 代表 $x_k$，则 $l(c_k)$ 减1。
    -   由于我们追求的是最优码（平均码长最小），任何可以进一步缩短平均码长的可能性都必须被排除。因此，最优前缀码对应的二叉树必须是“满的”，即 Kraft 不等式必须取等号。

> *(板书 (6:20 - 6:30, Part_2/3):*
>
> **2. Kraft's Ineq.** $\sum_{i=1}^{n} 2^{-|c_i|} = 1$ for opt. code

> *(教师在讲解 (约26:10 - 27:08) 时，通过画图 C1, C2, C3, C4, C5 的二叉树结构，说明了如果树不满，例如某个码字 C5 可以被缩短成 C5'（通过上提等操作），则平均码长会减小，与最优性矛盾。)*

### 3. 性质3：最长码字的特性 (Properties of Longest Codewords)

-   **结论 1 (等长性)：** 在最优码中，**概率最小的两个符号**（如果概率有并列最小，则从中任取两个）所对应的码字，其**长度必然相同**，并且这些码字是所有码字中**最长的之一**。
    假设我们已按概率降序排列符号，即 $p_1 \ge p_2 \ge \dots \ge p_{n-1} \ge p_n$。根据性质1， $l(c_1) \le l(c_2) \le \dots \le l(c_{n-1}) \le l(c_n)$。那么，对于概率最小的两个符号 $x_{n-1}$ 和 $x_n$ (对应的概率为 $p_{n-1}$ 和 $p_n$)，它们对应的码字 $c_{n-1}$ 和 $c_n$ 的长度 $l(c_{n-1})$ 和 $l(c_n)$ 不仅是所有码字中最长的（或并列最长），而且在某个最优码中必然有：
    $l(c_{n-1}) = l(c_n)$
-   **结论 2 (兄弟节点特性)：** 这两个码字 $c_{n-1}$ 和 $c_n$ (或者说，至少存在一个最优码，使得概率最小的两个符号对应的码字) **可以被安排成兄弟节点 (sibling nodes)**。这意味着它们在编码树中共享同一个父节点，并且仅在最后一位比特上不同（例如，一个以 `...0` 结尾，另一个以 `...1` 结尾）。
-   **解释与意义：**
    -   **关于等长性：** 如果 $l(c_n) > l(c_{n-1})$ (假设 $p_n$ 和 $p_{n-1}$ 是最小的两个概率，且 $p_{n-1} \ge p_n$)。由于 $c_n$ 是最长的码字之一，且根据性质2，编码树是满的，那么 $c_n$ 必须有一个兄弟节点（否则 $c_n$ 可以被缩短）。如果这个兄弟节点也是一个码字，它的长度与 $c_n$ 相同。如果 $c_{n-1}$ 不是 $c_n$ 的兄弟且 $l(c_n) > l(c_{n-1})$，我们可以考虑调整。更强的论证是，如果 $l(c_n) > l(c_{n-1})$，我们可以尝试将 $c_n$ 的前缀（长度为 $l(c_{n-1})$）分配给 $x_n$，并将 $c_{n-1}$ 分配给某个其他符号或进行调整，这往往能获得更优的码（这是一个复杂的交换论证，但直观上，不应该给概率更小或相等的符号分配严格更长的码字，如果它们都是最长码字群里的话）。
    -   **关于兄弟节点：** 如果概率最小的两个符号 $x_{n-1}$ 和 $x_n$ 对应的码字 $c_{n-1}$ 和 $c_n$ 具有相同的最长长度 $L_{max}$，但它们不是兄弟节点。这意味着它们在编码树的第 $L_{max}$ 层，但父节点不同。我们可以找到其中一个码字，比如 $c_n$。由于树是满的 (性质2)，$c_n$ 必定有一个兄弟节点 $c'_n$（也是长度为 $L_{max}$ 的叶节点或内部节点的分支）。如果 $c'_n$ 也是一个码字，且其概率 $p(c'_n) \ge p_n$，我们可以交换 $c_n$ 和 $c_{n-1}$ 中的一个与 $c'_n$ (如果 $p(c'_n) > p_{n-1}$，则与 $c_{n-1}$ 交换可能更优)。通过一系列这样的调整（可能涉及重新分配其他码字以保持前缀无关性），总能构造出一个平均码长不差于（甚至可能更优于）原码的新最优码，其中这两个概率最小的符号的码字成为兄弟节点。
    -   **对哈夫曼编码的启发：** 这个性质是哈夫曼编码算法中核心步骤“选择概率最小的两个符号，将它们合并为一个新的符号，其概率为两者之和”的直接理论依据。通过将它们视为兄弟节点，它们共享一个共同前缀，最后一位不同，这在构建编码树时自然发生。

> *(板书 (7:10 - 7:28, Part_3):*
>
> **3.** $|C_n| = |C_{n-1}| \implies C_n, C_{n-1}$ **are sibling nodes.**

> *(教师强调，这里 $C_n, C_{n-1}$ 代表概率最小的两个符号的码字。)*

> *(教师图示解释 (35:05 - 35:40, Part_3):* 教师曾画图示意，y轴表示码长 $l(c_i)$，x轴表示概率 $p_i$（通常习惯从左到右概率递减，即 $p_1, p_2, \dots, p_n$；但教师板书有时画成 $p_n, p_{n-1}, \dots, p_1$ 从左到右，即概率递增）。关键在于图示表明，概率最小的 $p_n$ 和 $p_{n-1}$ 对应的码长 $l(c_n)$ 和 $l(c_{n-1})$ 是相等的，并且是所有码字中的最大长度。

> *(例如，如果x轴从左到右是 $p_n, p_{n-1}, \dots, p_1$ (概率递增)，则 $l(c_n)=l(c_{n-1})$ 是最高点，然后码长随概率增大而非严格递减。)*

---

## 三、补充理论：熵的进一步理解 (源自 Lecture_3_Part_1)

### 1. 熵 (Entropy) 的多角度理解

-   **定义重申：** $H(X) = \sum_{i=1}^{n} p_i \log_2 \frac{1}{p_i}$ (比特)。

> *(板书 (05:09 - 05:13, Part_1):*
>
> **Entropy**
>
> - **r.v. X** pmf $P=(p_1, ..., p_n)$
> - $H(X) = \sum_{i=1}^{n} p_i \log_2 \frac{1}{p_i}$ (bits)

-   **信息量的度量：** $H(X)$ 量化了一个随机变量 $X$ 平均包含的信息量，代表无损压缩该随机变量平均所需的最小比特数（当允许码长为实数时）。
-   **不确定性 (Uncertainty) / 随机性 (Randomness) 的度量：**
    -   $H(X)$ 越大，随机变量 $X$ 的不确定性或随机性就越大。
    -   对于有 $n$ 个可能取值的 $X$，当其服从均匀分布 ($p_i = 1/n$ 对所有 $i$) 时，不确定性最大，$H(X) = \log_2 n$。
    -   一般有 $H(X) \le \log_2 n$ (可通过Jensen不等式证明，因 $\log x$ 是凹函数)。
-   **平均信息量 (自信息量的期望)：**
    -   定义单个消息 $m_i$ (对应 $X=x_i$) 的**自信息 (Self-information)** 为：
        $I(m_i) = \log_2 \frac{1}{p_i}$ (比特)
    -   熵 $H(X)$ 是各个消息自信息量的加权平均值（期望）：
        $H(X) = E[I(X)] = \sum_{i=1}^{n} p_i I(m_i)$
    -   直观上，一个事件发生的概率越小，它所包含的信息量（意外程度）就越大。

> *(板书 (07:08 - 07:19, Part_1):*
>
> **Information**
>
> - **r.v. X**: $H(X)$ (bits)
> - **message** $m_i$: $\log_2 \frac{1}{p_i}$ (bits)

### 2. 熵的可加性 (Additive Property of Entropy / Chain Rule for Entropy - 特殊情况)

此性质展示了当一个随机事件的某个结果被进一步细化时，总体熵如何变化。
-   **场景设定：**
    1.  随机变量 $X$ 的概率分布为 $P_X = (p_1, p_2, \dots, p_{n-1}, p_n)$。
    2.  假设当 $X$ 取其第 $n$ 个值 $x_n$ (概率为 $p_n$) 时，这一情况可以被进一步细分为两种子情况，记作随机变量 $Y$ 在条件 $X=x_n$ 下的取值。这两种子情况发生的（联合）概率分别为 $q_1$ 和 $q_2$，满足 $q_1 + q_2 = p_n$。
    3.  在已知 $X=x_n$ 的条件下，$Y$ 的条件概率分布为 $(\frac{q_1}{p_n}, \frac{q_2}{p_n})$。我们将这个条件下的熵记为 $H(Y|X=x_n)$。
    4.  我们构造一个新的随机变量 $Z$，它更精细地描述了整个样本空间。其概率分布为 $P_Z = (p_1, p_2, \dots, p_{n-1}, q_1, q_2)$。

> *(板书 (23:45 - 24:03 & 24:40, Part_1):*
>
> - **r.v. X:** $P_1, P_2, \dots, P_{n-1}, P_n (=q_1+q_2)$
> - **Y (given $X=x_n$):** pmf of Y is $(\frac{q_1}{P_n}, \frac{q_2}{P_n})$
> - **r.v. Z:** $P_1, P_2, \dots, P_{n-1}, q_1, q_2$

-   **可加性公式：**
    $H(Z) = H(X) + p_n H(Y|X=x_n)$

> *(教师有时简写为 $H(Z) = H(X) + p_n H(Y)$，其中 $H(Y)$ 特指在 $X=x_n$ 条件下的熵)*

> *(板书 (33:30, Part_1):*
>
> $H(X) + p_n H(Y) = H(Z)$

-   **推导：**
    $H(X) = \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} + p_n \log_2 \frac{1}{p_n}$
    $H(Y|X=x_n) = \frac{q_1}{p_n} \log_2 \frac{p_n}{q_1} + \frac{q_2}{p_n} \log_2 \frac{p_n}{q_2}$
    $H(Z) = \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} + q_1 \log_2 \frac{1}{q_1} + q_2 \log_2 \frac{1}{q_2}$

    代入公式左边 $H(X) + p_n H(Y|X=x_n)$:
    $= \left( \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} + p_n \log_2 \frac{1}{p_n} \right) + p_n \left( \frac{q_1}{p_n} \log_2 \frac{p_n}{q_1} + \frac{q_2}{p_n} \log_2 \frac{p_n}{q_2} \right)$
    $= \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} + p_n \log_2 \frac{1}{p_n} + q_1 (\log_2 p_n - \log_2 q_1) + q_2 (\log_2 p_n - \log_2 q_2)$
    使用 $\log(a/b) = \log a - \log b$ 和 $p_n \log_2 (1/p_n) = -p_n \log_2 p_n$:
    $= \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} - p_n \log_2 p_n + (q_1+q_2)\log_2 p_n + q_1 \log_2 \frac{1}{q_1} + q_2 \log_2 \frac{1}{q_2}$
    因为 $q_1+q_2 = p_n$:
    $= \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} - p_n \log_2 p_n + p_n \log_2 p_n + q_1 \log_2 \frac{1}{q_1} + q_2 \log_2 \frac{1}{q_2}$
    $= \sum_{i=1}^{n-1} p_i \log_2 \frac{1}{p_i} + q_1 \log_2 \frac{1}{q_1} + q_2 \log_2 \frac{1}{q_2}$
    这正好等于 $H(Z)$。
-   **意义：** 这个性质非常符合直觉：更精细描述 $Z$ 的总不确定性/信息量，等于描述 $X$ 的不确定性，再加上在 $X$ 的某个结果 $x_n$ 发生后，用于描述其子情况 $Y$ 的那部分额外的不确定性（按 $x_n$ 发生的概率加权）。这是更一般的熵链式法则 $H(X,Y) = H(X) + H(Y|X)$ 的一个具体体现。

---

## 四、总结与展望

本节课我们从编码问题的基本设定出发，探讨了一个最优（平均码长最短的）前缀无关码所必须具备的三个核心性质：
1.  **概率与码长的单调非增关系：** 高概率符号对应短码字。
2.  **Kraft不等式取等：** 最优码充分利用了码空间，其编码树是“满”的。
3.  **最长码字的特性：** 概率最小的两个符号，其码长相等且为最长之一，并且它们可以被安排为兄弟节点。

这些性质，特别是性质3，为我们设计具体的编码算法（如**哈夫曼编码 (Huffman Coding)**）提供了关键的洞察和理论依据。哈夫曼编码正是利用了迭代合并概率最小的两个符号（视作兄弟节点）来逐步构建最优编码树的思想。
接下来的课程将详细介绍哈夫曼编码算法及其最优性的证明。

---

## 五、课程管理与评分 (源自 Lecture_3_Part_1)

-   本课程**没有期中考试**。
-   最终成绩由以下部分构成：
    -   期末考试
    -   平时作业 (Homework)
    -   (可能) 大作业 (Final Project) - 具体安排视学生情况而定，会在后续几周内明确。
-   作业提交方式请与助教 (TA) 沟通。

---

## 六、布置的习题

在本节课的这些笔记片段中，教师**未明确布置具体的课后习题**。主要内容是引导学生理解和推导最优码的这些基本性质，鼓励学生思考这些性质的由来和意义。
