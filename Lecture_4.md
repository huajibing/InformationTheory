# 第4节：联合熵、条件熵、互信息与KL散度

## 〇、引言

上节课我们主要讨论了熵 ($$H(X)$$) 作为单个随机变量不确定性或信息量的基本度量。本节课将从熵的概念出发，扩展到多个随机变量的情况，引入联合熵、条件熵等重要概念。进一步，我们将探讨衡量随机变量之间相关性的核心工具——互信息，并揭示其与熵以及另一种重要的信息度量——Kullback-Leibler散度（相对熵）之间的深刻联系。这些概念是信息论中进行复杂系统分析和设计编码方案的基石。

---

## 一、联合熵 (Joint Entropy)

### 1. 定义:

对于两个离散随机变量 $$X$$ 和 $$Y$$，它们的联合概率质量函数为 $$P(X=x_i, Y=y_j)$$。其联合熵 $$H(X,Y)$$ 定义为：
$$
H(X,Y) = -\sum_{i}\sum_{j} P(X=x_i, Y=y_j) \log_2 P(X=x_i, Y=y_j) \quad (\text{比特})
$$
也可以简洁地写作：
$$
H(X,Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P(x,y) \log_2 P(x,y) \quad (\text{比特})
$$
> *(课堂板书亦有：$$H(X,Y) = -\sum_{i,j} P_{ij} \log_2 \frac{1}{P_{ij}} (\text{bits})$$，其中 $$P_{ij} = P(X=x_i, Y=y_j)$$，这里我们统一使用 $$\log_2 P(x,y)$$ 的形式，因为 $$\log_2 \frac{1}{P} = -\log_2 P$$）*

### 2. 理解:

-   联合熵 $$H(X,Y)$$ 衡量的是一对随机变量 $$(X,Y)$$ 作为一个整体（或一个向量随机变量）时的不确定性。它表示要描述 $$(X,Y)$$ 的平均信息量，或者说，对 $$(X,Y)$$ 进行最优编码所需要的平均比特数（在理想情况下）。

### 3. 性质:

-   **独立性**: 如果 $$X$$ 和 $$Y$$ 是相互独立的 (independent)，即 $$P(X,Y) = P(X)P(Y)$$，则：
$$
H(X,Y) = H(X) + H(Y)
$$
    - *推导*:
$$
H(X,Y) = -\sum_{x,y} P(x)P(y) \log_2 (P(x)P(y))
$$
$$
= -\sum_{x,y} P(x)P(y) (\log_2 P(x) + \log_2 P(y))
$$
$$
= -\sum_x P(x) \log_2 P(x) \sum_y P(y) - \sum_y P(y) \log_2 P(y) \sum_x P(x)
$$
        因为 $$\sum_y P(y) = 1$$ 和 $$\sum_x P(x) = 1$$:
$$
= -\sum_x P(x) \log_2 P(x) - \sum_y P(y) \log_2 P(y) = H(X) + H(Y)
$$
-   **完全相关性**: 如果 $$X$$ 和 $$Y$$ 是完全相关的 (perfectly correlated)，例如 $$X=Y$$，则：
$$
H(X,Y) = H(X) = H(Y)
$$
    因为此时 $$P(X,Y)$$ 只在 $$X=Y$$ 时非零，且 $$P(X=x, Y=x) = P(X=x)$$。

---

## 二、条件熵 (Conditional Entropy)

### 1. 特定条件下的条件熵:

给定随机变量 $$X$$ 取某一特定值 $$x_i$$ 时，随机变量 $$Y$$ 的条件熵 $$H(Y|X=x_i)$$ 定义为，在已知 $$X=x_i$$ 的条件下，$$Y$$ 的不确定性：
$$
H(Y|X=x_i) = -\sum_j P(Y=y_j|X=x_i) \log_2 P(Y=y_j|X=x_i) \quad (\text{比特})
$$
-   **理解**: 这个量衡量的是在已知 $$X=x_i$$ 这个具体信息后，$$Y$$ 仍然存在的不确定性，或者说，在 $$X=x_i$$ 的条件下对 $$Y$$ 进行最优编码所需的平均比特数。

### 2. 平均条件熵 (Conditional Entropy):

随机变量 $$Y$$ 关于随机变量 $$X$$ 的条件熵 $$H(Y|X)$$ 定义为 $$H(Y|X=x_i)$$ 在 $$X$$ 的所有可能取值上的期望（平均值）：
$$
H(Y|X) = \sum_i P(X=x_i) H(Y|X=x_i)
$$
代入 $$H(Y|X=x_i)$$ 的定义并展开：
$$
H(Y|X) = \sum_i P(X=x_i) \left( -\sum_j P(Y=y_j|X=x_i) \log_2 P(Y=y_j|X=x_i) \right)
$$
$$
H(Y|X) = -\sum_i \sum_j P(X=x_i) P(Y=y_j|X=x_i) \log_2 P(Y=y_j|X=x_i)
$$
由于 $$P(X=x_i) P(Y=y_j|X=x_i) = P(X=x_i, Y=y_j)$$，所以：
$$
H(Y|X) = -\sum_i \sum_j P(X=x_i, Y=y_j) \log_2 P(Y=y_j|X=x_i) \quad (\text{比特})
$$
-   **理解**: $$H(Y|X)$$ 表示在已知随机变量 $$X$$ 的取值后，随机变量 $$Y$$ 的平均不确定性。直观上，知道 $$X$$ 的信息通常会减少 $$Y$$ 的不确定性（除非它们独立）。

---

## 三、熵的链式法则 (Chain Rule for Entropy)

### 1. 两个随机变量的链式法则:

两个随机变量的联合熵可以分解为其中一个变量的熵加上另一个变量在该变量条件下的条件熵：
(1) $$H(X,Y) = H(X) + H(Y|X)$$
(2) $$H(X,Y) = H(Y) + H(X|Y)$$ (对称形式)

-   **推导 (以式(1)为例)**:
$$
H(X,Y) = -\sum_{x,y} P(x,y) \log_2 P(x,y)
$$
    使用 $$P(x,y) = P(x)P(y|x)$$:
$$
H(X,Y) = -\sum_{x,y} P(x,y) \log_2 (P(x)P(y|x))
$$
$$
= -\sum_{x,y} P(x,y) (\log_2 P(x) + \log_2 P(y|x))
$$
$$
= -\sum_{x,y} P(x,y) \log_2 P(x) - \sum_{x,y} P(x,y) \log_2 P(y|x)
$$
    对于第一项: $$\sum_{x,y} P(x,y) \log_2 P(x) = \sum_x (\sum_y P(x,y)) \log_2 P(x) = \sum_x P(x) \log_2 P(x) = -H(X)$$
    对于第二项: $$\sum_{x,y} P(x,y) \log_2 P(y|x) = \sum_x P(x) \sum_y P(y|x) \log_2 P(y|x)$$
$$
= \sum_x P(x) (-H(Y|X=x)) = -H(Y|X)
$$
    所以，$$H(X,Y) = H(X) + H(Y|X)$$。

> *(老师要求同学们课后自行推导，此处给出推导过程。)*

---

## 四、熵的基本性质

### 1. 联合熵与个体熵和的关系:

(3) $$H(X,Y) \leq H(X) + H(Y)$$
-   **理解**: 两个随机变量的联合熵不大于它们各自熵的和。等号成立当且仅当 $$X$$ 和 $$Y$$ 相互独立。
-   **证明**:
    由链式法则 $$H(X,Y) = H(X) + H(Y|X)$$。
    我们知道（或将在下面证明）$$H(Y|X) \le H(Y)$$。
    因此 $$H(X,Y) = H(X) + H(Y|X) \le H(X) + H(Y)$$。等号成立当 $$H(Y|X) = H(Y)$$，即 $$X$$ 和 $$Y$$ 相互独立。

### 2. 条件作用减少熵 (Conditioning Reduces Entropy):

(4) $$H(Y) \geq H(Y|X)$$
(5) $$H(X) \geq H(X|Y)$$ (对称形式)
-   **核心结论**: 条件作用（即知道一个变量的信息）通常会减少另一个相关变量的不确定性。等号成立当且仅当变量相互独立。
-   **证明 ($$H(Y) \ge H(Y|X)$$)**:
    该性质也称为信息非负性 $$I(X;Y) \ge 0$$ 的一个直接推论，因为 $$I(X;Y) = H(Y) - H(Y|X)$$。
    更直接的证明可以利用对数求和不等式或琴生不等式。
    考虑 $$H(Y) - H(Y|X)$$:
$$
H(Y) - H(Y|X) = -\sum_y P(y) \log_2 P(y) - (-\sum_{x,y} P(x,y) \log_2 P(y|x))
$$
$$
= \sum_{x,y} P(x,y) \log_2 P(y|x) - \sum_y (\sum_x P(x,y)) \log_2 P(y)
$$
$$
= \sum_{x,y} P(x,y) \log_2 P(y|x) - \sum_{x,y} P(x,y) \log_2 P(y)
$$
$$
= \sum_{x,y} P(x,y) \log_2 \frac{P(y|x)}{P(y)} = \sum_{x,y} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}
$$
    这个量即为互信息 $$I(X;Y)$$。我们稍后会证明 $$I(X;Y) \ge 0$$。

---

## 五、互信息 (Mutual Information)

### 1. 定义:

两个随机变量 $$X$$ 和 $$Y$$ 之间的互信息 $$I(X;Y)$$ 定义为：
$$
I(X;Y) \triangleq H(X) - H(X|Y)
$$
它也等价于：
$$
I(X;Y) = H(Y) - H(Y|X) \quad (\text{由对称性})
$$
并且还可以通过联合熵和个体熵表示为：
$$
I(X;Y) = H(X) + H(Y) - H(X,Y)
$$
-   *推导后者*: 从 $$H(X,Y) = H(X) + H(Y|X)$$ 出发，可得 $$H(Y|X) = H(X,Y) - H(X)$$。
    代入 $$I(X;Y) = H(Y) - H(Y|X)$$，得 $$I(X;Y) = H(Y) - (H(X,Y) - H(X)) = H(X) + H(Y) - H(X,Y)$$。

### 2. 理解:

-   $$H(X) - H(X|Y)$$: $$X$$ 的不确定性减去在已知 $$Y$$ 后 $$X$$ 的剩余不确定性。这个差值就是通过观察 $$Y$$ 所获得的关于 $$X$$ 的信息量。
-   $$H(Y) - H(Y|X)$$: 同理，表示通过观察 $$X$$ 所获得的关于 $$Y$$ 的信息量。
-   $$H(X) + H(Y) - H(X,Y)$$: $$X$$ 和 $$Y$$ 各自信息量之和，减去它们共同的信息量（即联合熵），剩下的就是它们共享的信息。可以类比集合论中的 $$|A \cup B| = |A| + |B| - |A \cap B|$$，这里熵扮演了类似“大小”的角色，而互信息则像“交集”。

### 3. 性质:

-   **对称性**: $$I(X;Y) = I(Y;X)$$。
-   **非负性**: $$I(X;Y) \ge 0$$。等号成立当且仅当 $$X$$ 和 $$Y$$ 相互独立。
>    （证明将在KL散度部分给出）
-   **与自身的关系**: $$I(X;X) = H(X) - H(X|X) = H(X) - 0 = H(X)$$。一个变量与自身包含的互信息就是它自身的熵。

### 4. 互信息的概率分布表达式 (课堂习题)

-   **问题**: 根据随机变量 $$X, Y$$ 的联合概率分布 $$P(X=x_i, Y=y_j)$$，推导出互信息 $$I(X;Y)$$ 的表达式。
-   **推导思路**: 从 $$I(X;Y) = H(X) + H(Y) - H(X,Y)$$ 出发，将各个熵的定义代入。
$$
H(X) = -\sum_i P(X=x_i) \log_2 P(X=x_i) = -\sum_i \left( \sum_j P(X=x_i, Y=y_j) \right) \log_2 P(X=x_i)
$$

$$
H(Y) = -\sum_j P(Y=y_j) \log_2 P(Y=y_j) = -\sum_j \left( \sum_i P(X=x_i, Y=y_j) \right) \log_2 P(Y=y_j)
$$

$$
H(X,Y) = -\sum_{i,j} P(X=x_i, Y=y_j) \log_2 P(X=x_i, Y=y_j)
$$

$$
I(X;Y) = H(X) + H(Y) - H(X,Y)
$$

$$
= -\sum_i \sum_j P(x_i,y_j) \log_2 P(x_i) - \sum_j \sum_i P(x_i,y_j) \log_2 P(y_j) + \sum_{i,j} P(x_i,y_j) \log_2 P(x_i,y_j)
$$
>    （使用了 $$P(x_i) = \sum_j P(x_i,y_j)$$ 和 $$P(y_j) = \sum_i P(x_i,y_j)$$，并将求和顺序统一）
$$
= \sum_{i,j} P(x_i,y_j) \left( \log_2 P(x_i,y_j) - \log_2 P(x_i) - \log_2 P(y_j) \right)
$$

$$
= \sum_{i,j} P(X=x_i, Y=y_j) \log_2 \frac{P(X=x_i, Y=y_j)}{P(X=x_i)P(Y=y_j)} \quad (\text{比特})
$$
-   这个表达式是互信息的一个非常重要的形式，它清晰地显示了互信息是联合分布 $$P(X,Y)$$ 与边缘分布乘积 $$P(X)P(Y)$$（即假设 $$X,Y$$ 独立时的联合分布）之间差异的度量。

---

## 六、熵概念的推广 (多个随机变量)

以上关于两个随机变量的熵、条件熵、链式法则和互信息的概念都可以自然地推广到多个随机变量。

### 1. 联合熵 (Joint Entropy) 的推广:

对于 $$n$$ 个随机变量 $$X_1, X_2, \dots, X_n$$，其联合概率分布为 $$P(x_1, x_2, \dots, x_n)$$，联合熵定义为：
$$
H(X_1, X_2, \dots, X_n) = -\sum_{x_1, \dots, x_n} P(x_1, x_2, \dots, x_n) \log_2 P(x_1, x_2, \dots, x_n) \quad (\text{比特})
$$
-   **理解**: 可将 $$X_1, \dots, X_n$$ 视为一个 $$n$$ 维向量随机变量。

### 2. 条件熵 (Conditional Entropy) 的推广:

对于两组随机变量 $$X_1, \dots, X_m$$ 和 $$Y_1, \dots, Y_n$$，条件熵 $$H(X_1, \dots, X_m | Y_1, \dots, Y_n)$$ 定义为：
$$
H(X_1, \dots, X_m | Y_1, \dots, Y_n) = H(X_1, \dots, X_m, Y_1, \dots, Y_n) - H(Y_1, \dots, Y_n)
$$
### 3. 互信息 (Mutual Information) 的推广:

对于两组随机变量 $$X_1, \dots, X_m$$ 和 $$Y_1, \dots, Y_n$$，互信息 $$I(X_1, \dots, X_m ; Y_1, \dots, Y_n)$$ 定义为：
$$
I(X_1, \dots, X_m ; Y_1, \dots, Y_n) = H(X_1, \dots, X_m) - H(X_1, \dots, X_m | Y_1, \dots, Y_n)
$$
### 4. 熵的链式法则 (Chain Rule for Entropy) 的推广:

对于 $$n$$ 个随机变量的联合熵，可以进行如下分解：
$$
H(X_1, X_2, \dots, X_n) = H(X_1) + H(X_2|X_1) + H(X_3|X_1,X_2) + \dots + H(X_n|X_1, X_2, \dots, X_{n-1})
$$
即：
$$
H(X_1, \dots, X_n) = \sum_{i=1}^{n} H(X_i | X_1, \dots, X_{i-1})
$$
其中，当 $$i=1$$ 时，$$H(X_1 | X_1, \dots, X_0)$$ 定义为 $$H(X_1)$$ (条件为空集)。
-   **理解**: 一组随机变量的联合不确定性，等于第一个变量的不确定性，加上在已知第一个变量后第二个变量的不确定性，依此类推。

---

## 七、Kullback-Leibler 散度 (KL-Divergence) / 相对熵 (Relative Entropy)

### 1. 引入背景 (实际编码问题):

-   假设我们有一个随机变量 $$X$$，其真实的概率分布是 $$P = \{p_1, p_2, \dots, p_n\}$$。
-   在实际中，这个真实分布 $$P$$ 往往是未知的。我们通常会有一个对 $$P$$ 的估计或近似分布 $$Q = \{q_1, q_2, \dots, q_n\}$$。
-   如果我们基于这个近似分布 $$Q$$ 来设计编码（例如，根据香农编码思想，第 $$i$$ 个消息的理想码长为 $$\log_2 \frac{1}{q_i}$$），那么实际传输这些消息时的平均码长是多少？
    由于消息实际出现的概率遵循真实分布 $$P$$，所以实际平均码长是根据 $$P$$ 来加权的：
    $$L_Q = \sum_i p_i \log_2 \frac{1}{q_i}$$ (使用 $$Q$$ 设计编码，按 $$P$$ 分布的平均码长)
-   **最优平均码长 (理论下界)**: 如果我们知道了真实分布 $$P$$，那么最优的平均码长（即熵）是：
$$
H(P) = \sum_i p_i \log_2 \frac{1}{p_i}
$$
### 2. KL 散度的定义:

KL 散度 (或称相对熵) $$D(P||Q)$$ 定义为由于使用近似分布 $$Q$$ 而非真实分布 $$P$$ 进行编码所带来的额外平均码长的增量：
$$
D(P||Q) = L_Q - H(P)
$$
$$
D(P||Q) = \sum_i p_i \log_2 \frac{1}{q_i} - \sum_i p_i \log_2 \frac{1}{p_i}
$$
$$
D(P||Q) = \sum_i p_i \left( \log_2 \frac{1}{q_i} - \log_2 \frac{1}{p_i} \right)
$$
$$
D(P||Q) = \sum_i p_i \log_2 \frac{p_i}{q_i} \quad (\text{比特})
$$
>    (也常记作 $$KL(P||Q)$$)
约定：$$0 \log_2 \frac{0}{q} = 0$$， $$p \log_2 \frac{p}{0} = \infty$$ (若 $$p>0, q=0$$)。

### 3. 性质:

-   **非负性**: $$D(P||Q) \ge 0$$。等号成立当且仅当 $$P=Q$$ (即 $$p_i = q_i$$ 对所有 $$i$$ 成立)。
    这个性质可以通过 **吉布斯不等式 (Gibbs' inequality)** 来证明。
    **吉布斯不等式**: 若 $$P=\{p_1, \dots, p_n\}$$ 和 $$Q=\{q_1, \dots, q_n\}$$ 是两个概率分布，则
$$
-\sum_i p_i \log_2 p_i \le -\sum_i p_i \log_2 q_i
$$
    即 $$H(P) \le \sum_i p_i \log_2 \frac{1}{q_i}$$。
    移项即得 $$D(P||Q) = \sum_i p_i \log_2 \frac{1}{q_i} - H(P) \ge 0$$。
    证明吉布斯不等式可用琴生不等式，利用 $$\ln x \le x-1$$ (或 $$\log_2 x \le (x-1)/\ln 2$$):
$$
D(P||Q) = \sum_i p_i \log_2 \frac{p_i}{q_i} = -\sum_i p_i \log_2 \frac{q_i}{p_i}
$$
$$
= -\frac{1}{\ln 2} \sum_i p_i \ln \frac{q_i}{p_i}
$$
    $$\ge -\frac{1}{\ln 2} \sum_i p_i \left( \frac{q_i}{p_i} - 1 \right)$$  (因为 $$\ln x \le x-1$$)
    $$= -\frac{1}{\ln 2} \sum_i (q_i - p_i) = -\frac{1}{\ln 2} (\sum_i q_i - \sum_i p_i) = -\frac{1}{\ln 2} (1 - 1) = 0$$.
-   **非对称性**: KL 散度不是一个严格意义上的距离度量，因为它不具有对称性，即通常情况下 $$D(P||Q) \neq D(Q||P)$$。这从其定义（哪个是真实分布 $$P$$，哪个是估计分布 $$Q$$）和物理意义中可以理解。

### 4. KL 散度的根本性:

在某些情况下，相对熵 (KL 散度) 比熵本身更为根本。例如，对于连续随机变量，其概率密度函数的熵可能为负无穷或没有良好定义，但两个概率密度函数之间的相对熵可能仍然是良好定义的。

---

## 八、互信息与 KL 散度的关系

回顾互信息的概率分布表达式：
$$
I(X;Y) = \sum_{x,y} P(X=x, Y=y) \log_2 \frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}
$$
将此式与 KL 散度的定义 $$D(P||Q) = \sum_i p_i \log_2 \frac{p_i}{q_i}$$ 对比：
我们可以将互信息 $$I(X;Y)$$ 视为联合概率分布 $$P(X,Y)$$ 与“$$X,Y$$ 独立时的联合分布” $$P(X)P(Y)$$ 之间的 KL 散度。
令 $$P'(x,y) = P(X=x, Y=y)$$ (作为 KL 散度中的“真实”分布)
令 $$Q'(x,y) = P(X=x)P(Y=y)$$ (作为 KL 散度中的“估计”或“参考”分布)
则：
$$
I(X;Y) = D(P(X,Y) || P(X)P(Y))
$$
-   **推论 (互信息的非负性)**:
    由于 KL 散度具有非负性 $$D(P||Q) \ge 0$$，所以互信息 $$I(X;Y) \ge 0$$。
    等号成立当且仅当 $$P(X,Y) = P(X)P(Y)$$ 对所有 $$x,y$$ 成立，即 $$X$$ 和 $$Y$$ 相互独立。这为互信息的非负性提供了一个严格的证明。

---

## 九、课堂思考/习题解答

### 1. 熵的凹凸性 (Convexity/Concavity of Entropy)

-   **问题**: 熵 $$H(P) = \sum_i p_i \log_2 \frac{1}{p_i} = -\sum_i p_i \log_2 p_i$$，将其视为概率分布 $$P = (p_1, \dots, p_n)$$ 的函数，$$H(P)$$ 关于 $$P$$ 是凸函数还是凹函数？
-   **解答**: 熵 $$H(P)$$ 是关于概率分布 $$P$$ 的 **凹函数 (concave function)**。
    -   **理由**: 函数 $$f(x) = -x \log_2 x$$ 对于 $$x \in (0,1]$$ 是一个凹函数（其二阶导数为 $$-1/(x \ln 2) < 0$$）。熵 $$H(P)$$ 是 $$n$$ 个此类凹函数的和 $$\sum_i f(p_i)$$，其中 $$(p_1, \dots, p_n)$$ 属于概率单纯形（即 $$p_i \ge 0, \sum p_i = 1$$）。在概率单纯形这个凸集上，凹函数的和仍然是凹函数。
    -   直观理解：均匀混合两个概率分布，得到的混合分布的熵通常不小于这两个分布熵的平均值，这正是凹函数的特性。例如，$$H(\lambda P_1 + (1-\lambda)P_2) \ge \lambda H(P_1) + (1-\lambda)H(P_2)$$。

### 2. KL 散度的凹凸性 (Convexity/Concavity of KL-Divergence)

-   考虑 KL 散度 $$D(P||Q) = \sum_i p_i \log_2 \frac{p_i}{q_i}$$。
-   **问题 (作为作业)**:
    a.  固定 $$Q$$， $$D(P||Q)$$ 作为 $$P$$ 的函数，是凸函数还是凹函数？
    b.  固定 $$P$$， $$D(P||Q)$$ 作为 $$Q$$ 的函数，是凸函数还是凹函数？
    c.  $$D(P||Q)$$ 作为 $$(P,Q)$$ 的联合函数，是凸函数还是凹函数？
-   **解答**:
    a.  **固定 $$Q$$， $$D(P||Q)$$ 作为 $$P$$ 的函数是凸函数 (convex in P)**。
        函数 $$f(x) = x \log x$$ 是凸函数。$$D(P||Q) = \sum_i q_i (\frac{p_i}{q_i} \log \frac{p_i}{q_i}) - \sum_i p_i \log q_i$$ (重写形式不易看出)。
        更标准地，可以证明相对熵是凸函数。一种方法是使用对数求和不等式 (log sum inequality) 或直接计算其Hessian矩阵并证明其为半正定。
    b.  **固定 $$P$$， $$D(P||Q)$$ 作为 $$Q$$ 的函数是凸函数 (convex in Q)**。
        这也可以通过对数求和不等式或计算Hessian矩阵来证明。
    c.  **$$D(P||Q)$$ 作为 $$(P,Q)$$ 的联合函数是凸函数 (jointly convex in (P,Q))**。
        这是 KL 散度的一个重要性质。

### 3. 不同距离/散度度量之间的关系

-   衡量两个概率分布 $$P$$ 和 $$Q$$ 之间差异的度量还包括：
    -   **$$L_1$$ 范数距离 / 总变差距离 (Total Variation Distance) 的一半**:
$$
||P-Q||_1 = \sum_i |p_i - q_i|
$$
        总变差距离定义为 $$TV(P,Q) = \frac{1}{2} \sum_i |p_i - q_i| = \frac{1}{2} ||P-Q||_1$$。
    -   **$$L_2$$ 范数距离 (欧氏距离的平方根)**:
$$
||P-Q||_2 = \sqrt{\sum_i (p_i - q_i)^2}
$$
-   **问题 (思考/探索)**: KL 散度 $$D(P||Q)$$ 与这些范数距离之间有什么关系？
-   **解答与讨论**:
    -   **Pinsker 不等式 (Pinsker's Inequality)**: 该不等式建立了 KL 散度与总变差距离之间的关系。对于以2为底的对数定义的 KL 散度，一个常见的形式是：
$$
TV(P,Q)^2 \le \frac{1}{2 \ln 2} D(P||Q)
$$
        或者写为：
$$
D(P||Q) \ge 2 (\ln 2) \cdot TV(P,Q)^2
$$
        这意味着如果 KL 散度很小，那么总变差距离也很小。反之不一定成立（总变差小不一定KL散度小，特别是当某个 $$q_i=0$$ 而 $$p_i > 0$$ 时，KL散度为无穷大，但总变差可能有限）。
        使用 $$||P-Q||_1 = 2 \cdot TV(P,Q)$$，则
$$
D(P||Q) \ge \frac{\ln 2}{2} ||P-Q||_1^2
$$
    -   **对于二元分布 ($$n=2$$)**:
        令 $$P=(p, 1-p)$$ 和 $$Q=(q, 1-q)$$。
$$
D(P||Q) = p \log_2 \frac{p}{q} + (1-p) \log_2 \frac{1-p}{1-q}
$$
$$
||P-Q||_1 = |p-q| + |(1-p)-(1-q)| = 2|p-q|
$$
$$
||P-Q||_2^2 = (p-q)^2 + ((1-p)-(1-q))^2 = 2(p-q)^2 \implies ||P-Q||_2 = \sqrt{2}|p-q|
$$
        Pinsker 不等式在此情况下变为：
$$
D(P||Q) \ge \frac{\ln 2}{2} (2|p-q|)^2 = 2 \ln 2 \cdot (p-q)^2
$$
        也可以找到 $$D(P||Q)$$ 与 $$(p-q)^2$$ 的其他界限，例如对于 $$|p-q|$$ 较小时，泰勒展开可得 $$D(P||Q) \approx \frac{1}{2 \ln 2} (\frac{(p-q)^2}{q(1-q)})$$.
    -   总的来说，KL 散度对分布的尾部差异或某个 $$q_i \approx 0$$ 的情况比 $$L_1$$ 或 $$L_2$$ 距离更敏感。

---

## 十、总结

本节课我们系统学习了信息论中描述多个随机变量及其相互关系的核心概念：
-   **联合熵 $$H(X,Y)$$**: 度量一对随机变量的总体不确定性。
-   **条件熵 $$H(Y|X)$$**: 度量在已知一个变量后，另一个变量的平均剩余不确定性，并引出了“条件作用减少熵”的重要性质。
-   **链式法则**: 将联合熵分解为一系列熵和条件熵之和。
-   **互信息 $$I(X;Y)$$**: 从不同角度（熵的减少量、熵的组合、概率分布的比较）度量两个变量共享的信息量或它们之间的依赖程度。
-   **Kullback-Leibler 散度 $$D(P||Q)$$ (相对熵)**: 量化了当使用一个估计分布 $$Q$$ 替代真实分布 $$P$$ 时所带来的信息损失或编码效率损失。其非负性是核心性质，且与互信息密切相关 ($$I(X;Y) = D(P(X,Y) || P(X)P(Y))$$), 由此证明了互信息的非负性。

这些概念的推广形式也适用于多个随机变量或变量组。最后，我们探讨了熵和 KL 散度的凹凸性，以及 KL 散度与其他距离度量（如 $$L_1, L_2$$ 范数）之间的关系，特别是 Pinsker 不等式。这些工具和理论为后续学习信源编码、信道编码以及更广泛的统计推断和机器学习应用奠定了坚实的基础。
