**《信息论》第10讲 课堂笔记：汉明码再探与信道容量**

**〇、引言**

在前几讲中，我们讨论了信源编码（如熵编码）、柯氏复杂度以及信道编码的初步概念，包括编码界和汉明码的引入。本节课我们将首先深入回顾和详解**汉明码 (Hamming Code)**，特别是其参数、校验矩阵、生成矩阵、编译码过程以及作为循环码和完美码的特性。随后，课程将转向一个核心且根本性的概念——**信道容量 (Channel Capacity)**。我们将探讨其定义、物理意义、计算方法（通过若干典型信道示例），并最终引出信息论中里程碑式的定理——**香农信道编码定理 (Shannon's Channel Coding Theorem)**。

---

**I. 汉明码 (Hamming Code) 深入回顾与详解**

汉明码是一类经典且重要的线性分组码，以其高效的纠错能力和简洁的代数结构著称。

1.  **基本参数与定义 (以 $(7,4,3)$ 汉明码为例)**
    *   **码长 (Length) $n=7$**: 每个码字由7个比特组成。
    *   **信息位数 (Dimension) $k=4$**: 每个码字承载4个比特的原始信息。因此，总共有 $2^k = 2^4 = 16$ 个不同的码字。
    *   **最小汉明距离 (Minimum Hamming Distance) $d_{min}=3$**: 任意两个不同的码字之间，对应位置上比特值不同的位数至少为3。
        *   纠错能力 $t = \lfloor (d_{min}-1)/2 \rfloor = \lfloor (3-1)/2 \rfloor = 1$。这意味着 $(7,4,3)$ 汉明码能够纠正任意单个比特的错误。
    *   这是一种**线性码 (Linear Code)**：任意两个码字的（模2）和仍然是一个码字；全零向量也是一个码字。码字集合构成 $\{0,1\}^n$ 空间的一个 $k$ 维线性子空间。

2.  **校验矩阵 (Parity-check Matrix) $H$**
    *   汉明码的码字集合 $C$ 是其校验矩阵 $H$ 的**零空间 (Null Space)**。对于 $(7,4,3)$ 汉明码，一个典型的校验矩阵 $H$ (定义在 $GF(2)$ 或 $F_2$ 上) 如下：
        板书:
        $$ H = \begin{pmatrix} 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 0 & 1 & 1 & 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 & 1 & 0 & 1 \end{pmatrix}_{3 \times 7} $$
        *   **结构说明**: 此矩阵的 $n-k=3$ 行是线性无关的，因此 $\text{rank}(H) = n-k = 3$。其列向量是所有非零的3比特向量的一种排列。例如，一种常见的排列方式是按二进制数值递增：$(001)^T, (010)^T, (011)^T, (100)^T, (101)^T, (110)^T, (111)^T$。板书所示的 $H$ 是这些列向量的另一种排列。
        *   为了形成系统码的校验矩阵，通常会将单位矩阵部分 $I_{n-k}$ 放在右侧（或左侧），形如 $H = [P | I_{n-k}]$ 或 $H = [I_{n-k} | P]$ (通过列置换实现)。例如，如果 $P = \begin{pmatrix} 001 \\ 010 \\ 011 \\ 100 \end{pmatrix}$，则 $H = [P^T | I_3]$ 是一个可能的系统码校验矩阵形式（需要对原始H的列进行重排）。
    *   **码字定义**:
        $C = \text{Null}(H) = \{x \in \{0,1\}^7 \mid Hx^T = \mathbf{0} \}$
        (注意：这里的向量 $x$ 通常指行向量，因此与 $H$ 相乘时需转置为 $x^T$。如果 $x$ 定义为列向量，则是 $Hx = \mathbf{0}$。运算均在 $GF(2)$ 上进行，即模2加法和模2乘法。)

3.  **生成矩阵 (Generator Matrix) $G$**
    *   生成矩阵 $G$ 的行向量构成了码空间 $C$ (即 $Null(H)$) 的一组基。$G$ 是一个 $k \times n$ (即 $4 \times 7$) 矩阵，其秩为 $k=4$。
    *   所有码字 $c$ 可以由一个 $1 \times k$ 的信息行向量 $s = (s_1, s_2, s_3, s_4)$ 生成：
        $c = s \cdot G$
    *   **标准型 (Standard Form) 或系统型 (Systematic Form) 生成矩阵 $G_{sys}$**:
        为了方便编码，通常采用系统型生成矩阵：
        $G_{sys} = [I_k | P]$
        其中 $I_k$ 是 $k \times k$ 的单位矩阵，$P$ 是一个 $k \times (n-k)$ 的矩阵。对于 $(7,4,3)$ 码，$G_{sys}$ 是一个 $4 \times 7$ 矩阵，形如 $G_{sys} = [I_4 | P_{4 \times 3}]$。
    *   **$G$ 与 $H$ 的关系**: 对于系统码 $G = [I_k | P]$，其对应的系统校验矩阵（经过合适的列置换）为 $H = [-P^T | I_{n-k}]$ (在 $GF(2)$ 中 $-P^T = P^T$)，即 $H = [P^T | I_{n-k}]$。它们满足 $G H^T = \mathbf{0}$ (零矩阵)。
    *   **编码 (Encoding) 示例**:
        若信息为 $s=(s_1, s_2, s_3, s_4)$，使用系统型 $G_{sys} = [I_4 | P]$ 编码：
        $c = s \cdot G_{sys} = s \cdot [I_4 | P] = [s \cdot I_4 | s \cdot P] = [s_1 s_2 s_3 s_4 | p_1 p_2 p_3]$
        这意味着码字的前 $k=4$ 位就是原始信息位，后 $n-k=3$ 位是根据信息位计算出的**校验位 (parity bits)**。
    *   **一个 $(7,4,3)$ 汉明码的非系统型生成矩阵示例 (来自 Part 1 笔记)**:
        $$ G = \begin{pmatrix}
        1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 1 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 & 1 & 0 & 1
        \end{pmatrix} $$
        这个 $G$ 的行向量是线性无关的，并且每个行向量都属于对应汉明码的零空间（可以验证 $g_i H^T = \mathbf{0}$，其中 $H$ 是与此 $G$ 对应的校验矩阵）。该 $G$ 产生的码是一个 $(7,4,3)$ 汉明码。

4.  **译码 (Decoding) 过程 (针对单比特错误)**
    *   接收端收到一个（可能带有噪声的）7比特序列 $x'$。
    *   计算**校验子 (Syndrome)** $S = x' H^T$。
        1.  **如果 $S = \mathbf{0}$ (全零向量)**:
            认为 $x'$ 没有错误 (或者发生的错误恰好将一个码字变成了另一个码字，但由于 $d_{min}=3$ 且我们假设最多1比特错误，这种情况不会与单比特错误混淆)。译码结果就是 $x'$ 的前 $k$ 位（如果采用系统码）。
        2.  **如果 $S \neq \mathbf{0}$**:
            这意味着 $x'$ 不是一个合法的码字，传输过程中发生了错误。假设发生了1个比特的错误，错误发生在第 $j$ 个位置。那么 $x' = c + e_j$，其中 $c$ 是原始码字，$e_j$ 是第 $j$ 位为1其余位为0的错误向量。
            $S = x' H^T = (c+e_j)H^T = c H^T + e_j H^T = \mathbf{0} + e_j H^T = e_j H^T$。
            $e_j H^T$ 正好等于校验矩阵 $H$ 的第 $j$ 行 (如果 $H$ 的列是 $h_1, \dots, h_n$，那么 $e_j H^T$ 等于 $H$ 的第 $j$ 列 $h_j^T$ 转置后的行向量；或者如果 $S=Hx'^T$, $S$ 将等于 $H$ 的第 $j$ 列)。
            **核心思想**: 如果校验子 $S$ 等于 $H$ 的第 $j$ 列（按列向量理解 $S$），则表明 $x'$ 的第 $j$ 位发生了翻转。
            **纠错**: 纠正 $x'$ 的第 $j$ 位（0变1，1变0），然后取前 $k$ 位作为译码结果（对于系统码）。
            **高效性**: 如果 $H$ 的列被排列成数字1到7的二进制表示，那么校验子 $S$ 本身的二进制值就直接指明了错误位置的序号。

5.  **完美码 (Perfect Code)**
    $(7,4,3)$ 汉明码是一个完美码，它满足**球堆积界限 (Sphere Packing Bound)** 的等号。
    球堆积界限为：$M \sum_{i=0}^{t} \binom{n}{i} \le 2^n$，其中 $M=2^k$ 是码字数量。
    对于 $(7,4,3)$ 汉明码：$n=7, k=4 \Rightarrow M=2^4=16, t=1$。
    $2^4 \left( \binom{7}{0} + \binom{7}{1} \right) = 16 \cdot (1+7) = 16 \cdot 8 = 128$。
    而 $2^n = 2^7 = 128$。
    由于 $128 = 128$，等号成立，故为完美码。这意味着以每个码字为中心、半径为 $t=1$ 的汉明球恰好无重叠地覆盖了整个 $\{0,1\}^7$ 空间。

6.  **扩展汉明码 (Extended Hamming Code)**
    *   可以将一个 $(n, k, d_{\text{odd}})$ 的线性码（其中 $d_{\text{odd}}$ 是奇数最小距离）通过增加一位**总体校验位 (overall parity check bit)** 扩展为一个 $(n+1, k, d_{\text{even}})$ 码，其中 $d_{\text{even}} = d_{\text{odd}} + 1$。
    *   方法：在每个原码字后附加一位校验位，使得新的 $(n+1)$ 比特码字总是有偶数个1 (even parity)。
    *   对于 $(7,4,3)$ 汉明码，可以扩展为 $(8,4,4)$ 码。
        *   新码长 $n'=8$，信息位数 $k'=4$ (校验位由信息位决定，不增加自由度)。
        *   原码字的重量分布为：1个（重量0），7个（重量3），7个（重量4），1个（重量7）。
        *   扩展后，奇数重量的码字（0, 3, 7）通过附加1使其变为偶数重量（0, 4, 8），偶数重量的码字（4）通过附加0保持偶数重量（4）。
        *   新码字的重量分布为：1个（重量0），7+7=14个（重量4），1个（重量8）。所有非零码字重量均为偶数，且最小非零重量为4。
        *   因此，扩展汉明码 $(8,4,4)$ 的最小距离 $d'_{min}=4$。它可以纠正1位错误并检测2位错误。

7.  **循环码 (Cyclic Code)**
    *   汉明码 $(7,4,3)$ 是一个**循环码**。
    *   **循环码的特性**：如果一个码字 $c = (c_0, c_1, \dots, c_{n-1})$ 属于一个循环码，那么它的任意循环移位 (cyclic shift) $c' = (c_{n-1}, c_0, \dots, c_{n-2})$ 也属于该码。
    *   许多重要的码如 BCH 码, Reed-Solomon 码都具有循环特性或其变种。

---

**II. 信道容量 (Channel Capacity)**

在了解了如何通过信道编码（如汉明码）来对抗信道噪声后，一个更根本的问题是：在一个有噪信道中，我们究竟能以多快的速率可靠地传输信息？这个速率的上限就是信道容量。

1.  **通信系统基本流程回顾**
    板书示意:
    $\text{原始信息} \xrightarrow{\text{信源编码}} \text{消息比特流} \xrightarrow{\text{信道编码}} \text{信道码字} \xrightarrow{\text{噪声信道}} \text{(可能损坏的)码字} \xrightarrow{\text{信道译码}} \text{恢复的消息比特流} \xrightarrow{\text{信源译码}} \text{恢复的原始信息}$
    *   上半学期主要讨论信源编码 (Source Coding)，其目标是去除冗余，高效表示信息，与信息熵 $H(X)$ 密切相关。
    *   下半学期主要讨论信道编码 (Channel Coding) 与信道容量，其目标是引入冗余，对抗噪声，实现可靠传输。

2.  **引入：从熵到信道容量 (源自Part 2)**
    *   **熵 (Entropy, $shāng$) 的命名**: 老师提及香农在定义了信息熵的数学形式后，曾咨询冯·诺依曼 (John von Neumann) 如何命名。冯·诺依曼建议使用“熵”，理由有二：
        1.  其数学形式与物理学中的熵（如玻尔兹曼熵）非常相似。
        2.  “熵”这个概念本身比较晦涩，“没人真正知道熵是什么 (nobody knows what entropy really is)，所以在辩论中你总能占有优势。” (香农转述冯·诺依曼的话)
    *   **信道容量 (Channel Capacity, $xìndào róngliàng$)**: 与熵的概念部分借鉴于物理学不同，信道容量是香农的一项纯粹的、原创性的核心贡献，它定量描述了噪声信道传输信息的根本极限。

3.  **噪声信道建模与互信息**
    *   **问题1：如何形式化描述一个噪声信道？**
        板书:
        `Noisy Channel`
        $X \longrightarrow Y$
        `1. How to formulate a noisy channel?`
        *   输入信号 $X$ (随机变量) 经过噪声信道后，输出变为 $Y$ (随机变量)。
        *   香农的通用描述：噪声信道的特性由**条件概率分布 (conditional probability distribution) $P(Y|X)$** 完全刻画。它描述了当输入为某个特定值 $x$ 时，观察到输出为 $y$ 的概率 $P(Y=y|X=x)$。
        *   (注：简单的加性噪声模型 $Y = X + e$ 只是 $P(Y|X)$ 的一种特例，不具有通用性。)
    *   **问题2：给定一个噪声信道 $P(Y|X)$，接收方通过观测 $Y$ 能获得多少关于原始输入 $X$ 的信息？**
        *   这个问题的答案是**互信息 (Mutual Information) $I(X;Y)$**。
            板书: $I(X;Y)$
        *   互信息度量了随机变量 $Y$ 中包含的关于随机变量 $X$ 的信息量 (反之亦然，因其对称性)。
        *   常用定义： $I(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y)$。
        *   也可用KL散度定义： $I(X;Y) = D(P_{XY} || P_X P_Y)$，其中 $P_{XY}$ 是联合分布，$P_X, P_Y$ 是边缘分布。

4.  **信道容量的定义**
    板书: `Def (Channel Capacity)`
    *   信道本身由固有的 $P(Y|X)$ 描述。
    *   发送方可以通过改变输入 $X$ 的概率分布 $P_X$ 来尝试优化信息传输。
    *   互信息 $I(X;Y)$ 的大小不仅取决于信道特性 $P(Y|X)$，还取决于输入分布 $P_X$。
    *   **信道容量 $C$ (Channel Capacity)** 定义为：在所有可能的输入概率分布 $P_X$ 中，所能达到的互信息 $I(X;Y)$ 的最大值。
        板书: $$ C = \max_{P_X} I(X;Y) $$

5.  **信道容量的意义**
    *   $C$ 代表了这个信道传输信息的固有能力上限，是信道本身的属性。
    *   其单位通常是比特/信道使用 (bits per channel use)。
    *   香农的**信道编码定理**（稍后详述）表明，$C$ 是在该信道上能够实现可靠通信（即错误概率任意小）的最大信息传输速率。

6.  **信道容量示例计算**

    *   **(a) 无噪信道 (Noiseless Channel)** (Part 2)
        *   输入 $X \in \{0,1\}$, 输出 $Y \in \{0,1\}$。
        *   信道特性: $X=0 \Rightarrow Y=0$ (概率1), $X=1 \Rightarrow Y=1$ (概率1)。
            板书:
            $X \quad Y$
            $0 \longrightarrow 0$
            $1 \longrightarrow 1$
        *   此时 $Y=X$，所以 $H(Y|X) = 0$。
        *   $I(X;Y) = H(Y) - H(Y|X) = H(Y)$。
        *   由于 $Y=X$, $H(Y)=H(X)$。所以 $I(X;Y) = H(X)$。
        *   $C = \max_{P_X} I(X;Y) = \max_{P_X} H(X)$。
        *   对于二元输入，当 $P(X=0) = P(X=1) = 1/2$ 时，$H(X)$ 达到最大值 $H(1/2, 1/2) = 1$ 比特。
            板书: $C = \max_{P_X} H(X) = 1 \text{ bit}$

    *   **(b) 完全噪声/无用信道 (Completely Noisy / Useless Channel)** (Part 2)
        *   输入 $X \in \{0,1\}$, 输出 $Y \in \{0,1\}$。
        *   信道特性: 输出 $Y$ 的分布与输入 $X$ 无关。例如，$P(Y=0|X=0) = P(Y=1|X=0) = 1/2$，$P(Y=0|X=1) = P(Y=1|X=1) = 1/2$。这意味着 $P(Y|X) = P(Y)$。
        *   此时 $X$ 和 $Y$ 相互独立。
        *   $I(X;Y) = 0$。
        *   $C = \max_{P_X} I(X;Y) = \max_{P_X} 0 = 0 \text{ bit}$。

    *   **(c) Z信道/确定性比特翻转信道 (Bit-flipping Channel)** (Part 3 提及)
        *   输入 $X \in \{0,1\}$, 输出 $Y \in \{0,1\}$。
        *   信道特性: $X=0 \rightarrow Y=1$ (概率1), $X=1 \rightarrow Y=0$ (概率1)。
        *   虽然输出与输入相反，但存在确定的一一对应关系。接收方只需将接收到的比特翻转即可完美恢复输入。
        *   $H(Y|X) = 0$ (因为给定 $X$, $Y$ 是确定的)。
        *   $I(X;Y) = H(Y)$。
        *   与无噪信道类似，当 $P(X=0)=P(X=1)=1/2$ 时，$P(Y=0)=P(Y=1)=1/2$，$H(Y)=1$。
        *   $C = 1 \text{ bit}$。

    *   **(d) 四输入二输出的确定性信道 (Example 2 from Part 3)**
        *   输入 $X \in \{A_1, A_2, B_1, B_2\}$。
        *   输出 $Y \in \{A, B\}$。
        *   信道特性 (确定性映射):
            *   若 $X=A_1$ 或 $X=A_2$，则 $Y=A$ (概率为1)。
            *   若 $X=B_1$ 或 $X=B_2$，则 $Y=B$ (概率为1)。
            板书 (示意图):
            $X \quad Y$
            $A_1 \searrow$
            $\quad\quad A$
            $A_2 \nearrow$
            $B_1 \searrow$
            $\quad\quad B$
            $B_2 \nearrow$
        *   **求解容量 $C$**:
            *   $H(Y|X) = 0$ (因为给定 $X$, $Y$ 是确定的)。
            *   $I(X;Y) = H(Y) - H(Y|X) = H(Y)$。
            *   $C = \max_{P_X} H(Y)$。
            *   $Y$ 只能取两个值 $A, B$。$H(Y)$ 的最大值为 $H(1/2, 1/2) = 1$ 比特，当 $P(Y=A) = P(Y=B) = 1/2$ 时达到。
            *   要使 $P(Y=A)=1/2$，需要 $P(X=A_1) + P(X=A_2) = 1/2$。
            *   要使 $P(Y=B)=1/2$，需要 $P(X=B_1) + P(X=B_2) = 1/2$。
            *   存在很多输入分布 $P_X$ 可以满足此条件。例如，令 $P(X=A_1)=1/2, P(X=A_2)=0, P(X=B_1)=1/2, P(X=B_2)=0$。
            *   因此，该信道的容量 $C = 1 \text{ bit}$。
            板书: $C=1 \text{ bit}$
            *   **直观解释**: 这个信道等效于一个无噪的二元信道。如果我们将 $A_1, A_2$ 视为发送“逻辑A”，$B_1, B_2$ 视为发送“逻辑B”，那么接收端完美知道发送的是“逻辑A”还是“逻辑B”。

    *   **(e) 26字母信道，一个输入有噪声 (Example 3 initial from Part 3)**
        *   输入 $X \in \{'A', \dots, 'Z'\}$ (26个字母)。
        *   输出 $Y \in \{'A', \dots, 'Z'\}$。
        *   信道特性:
            *   若 $X='A'$，则 $Y='A'$ (概率 $1/2$)，$Y='B'$ (概率 $1/2$)。
            *   若 $X=x$ 且 $x \in \{'B', \dots, 'Z'\}$，则 $Y=x$ (概率1)。
        *   **计算 $H(Y|X)$**:
            *   $H(Y|X='A') = H(1/2, 1/2) = 1$ 比特。
            *   $H(Y|X=x)$ for $x \neq 'A'$ is $H(1,0,\dots,0) = 0$ 比特。
            *   $H(Y|X) = \sum_x P(X=x) H(Y|X=x) = P(X='A') \cdot 1 + \sum_{x \neq 'A'} P(X=x) \cdot 0 = P(X='A')$。
            板书: $H(Y|X) = P(X='A')$
        *   **计算 $I(X;Y)$**:
            *   $I(X;Y) = H(Y) - H(Y|X) = H(Y) - P(X='A')$。
        *   **最大化 $I(X;Y)$**:
            *   直观上，我们希望 $P(X='A')$ 尽可能小（例如趋近于0），这样 $H(Y|X)$ 就小。
            *   如果令 $P(X='A') = 0$，并且其他25个字母等概率出现，即 $P(X=x) = 1/25$ for $x \neq 'A'$。
            *   此时 $Y$ 的分布是：$P(Y=x) = 1/25$ for $x \in \{'B', \dots, 'Z'\}$，$P(Y='A')=0$。
            *   $H(Y) = H(0, 1/25, \dots, 1/25) = \log_2 25$ 比特。
            *   $H(Y|X) = P(X='A') = 0$。
            *   $I(X;Y) = \log_2 25 - 0 = \log_2 25 \approx 4.64$ 比特。
            *   这应该是最大值，因此 $C = \log_2 25$ 比特。
            *(老师在口述中意识到这个例子计算一般情况下的 $H(Y)$ 比较复杂，随后转向了经典的BSC模型。)*

    *   **(f) 二元对称信道 (Binary Symmetric Channel, BSC)** (Example 3 classic from Part 3)
        *   输入 $X \in \{0,1\}$，输出 $Y \in \{0,1\}$。
        *   信道特性 (参数 $\epsilon$ 为错误概率, $0 \le \epsilon \le 1$):
            *   $P(Y=0|X=0) = 1-\epsilon$
            *   $P(Y=1|X=0) = \epsilon$
            *   $P(Y=0|X=1) = \epsilon$
            *   $P(Y=1|X=1) = 1-\epsilon$
            板书 (信道转移概率矩阵和图示):
            $\quad \quad 0 \quad 1 \quad (Y)$
            $0 \quad (X) \quad 1-\epsilon \quad \epsilon$
            $1 \quad \quad \quad \epsilon \quad \quad 1-\epsilon$

            $0 \xrightarrow{1-\epsilon} 0 \quad;\quad 0 \xrightarrow{\epsilon} 1$
            $1 \xrightarrow{\epsilon} 0 \quad;\quad 1 \xrightarrow{1-\epsilon} 1$
        *   **计算 $H(Y|X)$**:
            *   $H(Y|X=0) = H_b(\epsilon) = -\epsilon \log_2 \epsilon - (1-\epsilon) \log_2 (1-\epsilon)$ (二元熵函数)。
            *   $H(Y|X=1) = H_b(\epsilon)$。
            *   $H(Y|X) = \sum_x P(X=x) H(Y|X=x) = P(X=0)H_b(\epsilon) + P(X=1)H_b(\epsilon) = H_b(\epsilon)$ (因为 $H(Y|X=x)$ 与 $x$ 无关)。
            板书: $H(Y|X) = H_b(\epsilon)$
        *   **计算 $I(X;Y)$**:
            *   $I(X;Y) = H(Y) - H(Y|X) = H(Y) - H_b(\epsilon)$。
        *   **最大化 $I(X;Y)$**:
            *   为最大化 $I(X;Y)$，需最大化 $H(Y)$。$H(Y)$ 的最大值为1比特 (当 $P(Y=0) = P(Y=1) = 1/2$ 时)。
            *   当输入分布为均匀分布 $P(X=0)=P(X=1)=1/2$ 时：
                $P(Y=0) = P(X=0)P(Y=0|X=0) + P(X=1)P(Y=0|X=1) = (1/2)(1-\epsilon) + (1/2)(\epsilon) = 1/2$。
                同理 $P(Y=1)=1/2$。
            *   因此，当 $P_X$ 为均匀分布时，$H(Y)=1$。
        *   **信道容量 $C$**:
            $C = \max_{P_X} I(X;Y) = 1 - H_b(\epsilon)$。
            板书: $C = 1 - H_b(\epsilon)$
        *   **讨论不同 $\epsilon$ 值下的 $C$**:
            *   若 $\epsilon = 0$ (无噪信道)：$H_b(0)=0 \Rightarrow C = 1-0 = 1$ 比特。
            *   若 $\epsilon = 1/2$ (完全噪声信道)：$H_b(1/2)=1 \Rightarrow C = 1-1 = 0$ 比特。
            *   若 $\epsilon = 1$ (Z信道，比特恒翻转)：$H_b(1)=0 \Rightarrow C = 1-0 = 1$ 比特。

---

**III. 香农信道编码定理 (Shannon's Channel Coding Theorem)**

在定义并计算了信道容量 $C$ 之后，其最重要的意义体现在香农第二定理，即信道编码定理中。

1.  **非正式陈述 (Informal Statement from Part 3)**
    板书:
    `Channel Coding Theorem (Informal)`
    `Error Correcting Code (ECC):`
    `  e.g., 0 ----> 00000`
    `        1 ----> 11111`
    `R: rate, number of (information) bits transmitted per channel use`
    `If R < C (channel capacity)`
    $\quad \Rightarrow \exists \text{ ECC s.t. error probability } P_e \to 0 \text{ (as block length } N \to \infty)$
    `If R > C`
    $\quad \Rightarrow \forall \text{ ECC, } P_e \text{ cannot be arbitrarily small (bounded away from 0)}$
    $\quad (\text{i.e., } \exists \epsilon_0 > 0, \forall \text{ ECC } \Rightarrow P_e \ge \epsilon_0)$

2.  **定理核心思想**:
    *   **速率 (Rate) $R$**: 衡量的是每次使用信道（即传输一个信道符号）时，平均能传输多少个原始信息比特。对于分组码，如果将 $k$ 个信息比特编码成 $n$ 个信道符号，则速率 $R=k/n$。
    *   **信道容量 (Channel Capacity) $C$**: 是信道的固有属性，代表了信息传输能力的上限。
    *   **定理内容**:
        *   **正向部分 (Achievability)**: 如果信息传输速率 $R$ 严格小于信道容量 $C$ ($R < C$)，那么理论上存在一系列纠错码 (Error Correcting Codes, ECC) 及其相应的编解码方案，使得随着码长 $N$ 趋于无穷大，传输的错误概率 $P_e$ 可以任意小 (趋近于0)。这意味着可以实现**可靠通信 (reliable communication)**。
        *   **反向部分 (Converse)**: 如果信息传输速率 $R$ 大于信道容量 $C$ ($R > C$)，那么无论采用何种纠错码，都不可能使错误概率任意小。错误概率会有一个大于零的下界 ($\epsilon_0$)。这意味着无法实现可靠通信。
    *   **重要性**:
        *   该定理是信息论的基石之一，它指出了在有噪信道中实现可靠通信的可能性和根本极限。
        *   它表明，噪声并非不可克服的障碍，只要传输速率不超过信道容量，就可以通过足够复杂的编码达到极高的可靠性。
        *   这个结论通常是在码长 $N \to \infty$ 的渐进情况下成立的。香农的证明是存在性的，并未直接给出构造这类好码的具体方法。

---

**IV. 课堂练习与思考题 (源自 Part 1)**

1.  **练习**: 写出汉明码 $(7,4,3)$ 的全部 $2^4=16$ 个码字。
    *   **提示**: 可以使用一个生成矩阵 $G$ (例如 Part 1 笔记 Section 6 给出的 $G$，或从标准型 $H$ 导出的系统型 $G$)。将所有16个可能的4比特信息 $s = (s_1, s_2, s_3, s_4)$ 通过 $c = s \cdot G$ 映射到码字。
    *   **解答**: 使用 Part 1 笔记 Section 6 给出的生成矩阵：
        $G = \begin{pmatrix}
        1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 1 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 & 1 & 0 & 1
        \end{pmatrix}$
        令 $g_1, g_2, g_3, g_4$ 为 $G$ 的行向量。16个码字是 $s_1g_1 + s_2g_2 + s_3g_3 + s_4g_4$ 的所有组合：
        *   $0000 \rightarrow \mathbf{0000000}$
        *   $1000 \rightarrow \mathbf{1101000}$ ($g_1$)
        *   $0100 \rightarrow \mathbf{0110100}$ ($g_2$)
        *   $0010 \rightarrow \mathbf{0011010}$ ($g_3$)
        *   $0001 \rightarrow \mathbf{0001101}$ ($g_4$)
        *   $1100 \rightarrow \mathbf{1011100}$ ($g_1+g_2$)
        *   $1010 \rightarrow \mathbf{1110010}$ ($g_1+g_3$)
        *   $1001 \rightarrow \mathbf{1100101}$ ($g_1+g_4$)
        *   $0110 \rightarrow \mathbf{0101110}$ ($g_2+g_3$)
        *   $0101 \rightarrow \mathbf{0111001}$ ($g_2+g_4$)
        *   $0011 \rightarrow \mathbf{0010111}$ ($g_3+g_4$)
        *   $1110 \rightarrow \mathbf{1000110}$ ($g_1+g_2+g_3$)
        *   $1101 \rightarrow \mathbf{1010001}$ ($g_1+g_2+g_4$)
        *   $1011 \rightarrow \mathbf{1111111}$ ($g_1+g_3+g_4$) (全1码字)
        *   $0111 \rightarrow \mathbf{0100011}$ ($g_2+g_3+g_4$)
        *   $1111 \rightarrow \mathbf{1001011}$ ($g_1+g_2+g_3+g_4$)

    *   **观察1: 验证这些码字是否满足循环码的特性。**
        *   **解答**: 是的，$(7,4,3)$ 汉明码是循环码。这意味着对上述任一码字进行循环右移（或左移），得到的向量仍是这16个码字之一。
            例如，码字 $g_1 = (1101000)$。循环右移得到 $(0110100)$，这是 $g_2$。
            码字 $g_2 = (0110100)$。循环右移得到 $(0011010)$，这是 $g_3$。
            码字 $g_3 = (0011010)$。循环右移得到 $(0001101)$，这是 $g_4$。
            码字 $g_4 = (0001101)$。循环右移得到 $(1000110)$，这是 $g_1+g_2+g_3$。
            这个性质对所有16个码字都成立。其生成多项式为 $g(x) = 1+x+x^3$ (或其互反多项式，取决于定义)，校验多项式为 $h(x)=(x^7-1)/g(x) = 1+x+x^2+x^4$。

    *   **观察2: 观察这些码字的汉明重量 (weight) 分布。**
        *   **解答**:
            *   重量0: 1个 ($\mathbf{0000000}$)
            *   重量3: 7个
                ($g_1, g_2, g_3, g_4$,
                $g_1+g_2+g_3$, $g_1+g_2+g_4$, $g_2+g_3+g_4$)
            *   重量4: 7个
                ($g_1+g_2, g_1+g_3, g_1+g_4, g_2+g_3, g_2+g_4, g_3+g_4$,
                $g_1+g_2+g_3+g_4$)
            *   重量7: 1个 ($g_1+g_3+g_4 = \mathbf{1111111}$)
            总计 $1+7+7+1 = 16$ 个码字。这是 $(7,4,3)$ 汉明码的标准重量分布。

    *   **观察3: 验证任意两个不同码字之间的汉明距离至少为3。**
        *   **解答**: 由于这是线性码，任意两个不同码字 $c_i, c_j$ 之间的汉明距离 $d_H(c_i, c_j)$ 等于 $w_H(c_i \oplus c_j)$。而 $c_i \oplus c_j$ 也是一个非零码字。因此，最小汉明距离 $d_{min}$ 等于码中非零码字的最小汉明重量 $w_{min}$。
        *   从上述重量分布可知，最小的非零码字重量是3。因此，$d_{min}=3$。这意味着任意两个不同码字之间的汉明距离至少为3。

---

**V. 总结**

本节课（整合Part 1, 2, 3）首先详细回顾并深化了对**汉明码**的理解，包括其 $(7,4,3)$ 实例的参数、校验矩阵与生成矩阵、系统码形式、高效编译码方法、作为完美码和循环码的特性，以及如何扩展为 $(8,4,4)$ 码。

随后，课程引入了信息论下半场的关键概念——**信道容量 $C$**。
1.  通过熵的命名趣闻，引出了香农原创的信道容量概念。
2.  阐述了噪声信道的通用数学模型（条件概率 $P(Y|X)$）和衡量信息传递量的核心工具（互信息 $I(X;Y)$）。
3.  正式定义了信道容量为 $C = \max_{P_X} I(X;Y)$，并解释了其物理意义。
4.  通过一系列经典示例（无噪信道、完全噪声信道、确定性翻转信道、一个四输入二输出的确定性信道、以及最重要的二元对称信道BSC）计算并解释了信道容量。BSC的容量为 $C = 1 - H_b(\epsilon)$。
5.  最后，非正式地介绍了**香农信道编码定理**，阐明了信道容量 $C$ 作为在噪声信道中实现可靠通信（即可使错误概率任意小）的信息传输速率 $R$ 的理论上限。

这为后续深入学习信道编码理论和更高级的编码技术奠定了坚实的基础。