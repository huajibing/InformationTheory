# 第2节：信息量化、编码与熵

## 〇、引言

本节课程承接信息论的核心议题，深入探讨如何量化信息以及如何通过高效编码实现信息在有噪信道中的可靠传输。核心目标是理解信息量的度量方法，并在此基础上研究最优编码方案，特别是引入熵作为信息量的基本度量和数据压缩的理论极限。

---

## 一、课程导入与核心问题回顾

-   **信息论的核心内容：**
    信息论 (Information Theory) 的核心研究内容是**通信 (Communication)**。教师提及，“信息论”是一个更“时髦”(Fancy) 的名称，其本质与人工智能 (AI) 这类概念相似，旨在为通信的根本问题提供理论框架。

-   **信息论研究的两个基本问题：**
    1.  **信息量的量化 (Quantify Information):** 如何科学地衡量信息本身的大小或数量？这是后续讨论传输效率和极限的基础。
    2.  **有噪信道传输与纠错 (Noisy Channel Coding):** 在一个存在噪声的物理信道 (channel) 中，如何设计算法，特别是利用纠错码 (Error Correcting Code, ECC)，来克服噪声、纠正错误，并确定信息传输速率的理论极限。

---

## 二、信息量的量化与编码基础

### 1. 量化信息量的需求：

为了讨论信息传输的效率和极限，首要任务是建立一种方法来衡量信息的大小或数量。这是信息论最基本的问题之一。

### 2. 编码示例：足球比赛结果

**情景：** 假设需要传输一场足球比赛的结果。可能的结果有三种：胜 (Win, W)、负 (Lose, L)、平 (Tie, T)。
**待编码的 Message (信息源符号集)：** $\mathcal{X} = \{W, L, T\}$
**编码所用的 Alphabet (码字符号集)：** 通常为二进制字符集 $\mathcal{A} = \{0, 1\}$
**目标：** 使用最少的比特数 (bits) 来有效地编码这三种信息。

**编码方案1 (教师板书，非唯一可译)：**

> *(教师板书，非唯一可译)：*

$W \rightarrow 0$
$L \rightarrow 1$
$T \rightarrow 10$

**问题：** 这种编码方式是否可行？

**讨论：** 若只传输一次结果，该编码没有歧义。但若连续传输多次结果（例如，传输序列 $WL$），编码为 $01$。接收方收到 $01$ 时，无法判断原始序列是 $WL$（$0$ 后跟 $1$）还是 $W$ 后跟一个 $L$ 对应的 $1$ (如果 $W$ 也可以是 $01$ 的前缀，或者 $L$ 被编码为 $1$ 而 $T$ 是其他以 $1$ 开头的码字）。更准确地说，如果发送 $W L \rightarrow 01$，而 $T \rightarrow 10$，那么序列 $010$ 可能是 $W T$ ($0$ 和 $10$)，也可能是 $W L W$ ($0, 1, 0$)，如果 $W \to 0, L \to 1$。此例中，$01$ 可能被解读为 $WL$。若之后是 $T$，则 $0110$。但如果存在一个码字是另一个码字的前缀，例如 $W \to 0, L \to 01$，则 $001$ 可以是 $WWL$ 或 $W L'$ (若 $L' \to 01$)。
            在这个具体例子中 $W \to 0, L \to 1, T \to 10$，序列 $WL \to 01$。如果想发送 $WT$，则为 $010$。但 $010$ 也可以被解析为 $W$ ($0$) 加上 $L$ ($1$) 加上 $W$ ($0$)，即 $WLW \to 010$。因此，接收端会产生混淆。这种编码不具备**唯一可译性 (Uniquely Decodable)**，尤其是在连续传输时。

### 3. 信息论中的基本假设

> *(教师板书)：*

#### 1. 无限次通信 (Infinite communication):

信息论中的许多理论，特别是关于信道容量和编码极限的结论，是基于发送方和接收方进行大量乃至无限次信息传输的假设。这是为了能够分析平均特性和达到理论极限性能。

#### 2. 消息的概率分布 (A probability distribution over the messages):

假设待传输的各个消息（如 $W, L, T$）是根据一个已知的概率分布 $P = \{p_W, p_L, p_T\}$ 产生的。
**教师强调：** 现实中，这个概率分布可能未知，或者随时间变化，这会引入额外的复杂性（如通用编码问题）。但经典信息论通常假设该分布已知且固定。

### 4. 编码的核心目标

> *(教师板书)：*

**Goal: Minimize average code length.** (最小化平均码长)
在满足唯一可译性的前提下，这是衡量编码效率的核心指标。平均码长 $L_{avg}$ 定义为：
$$ L_{avg} = \sum_{x \in \mathcal{X}} p(x) l(x) $$
其中 $p(x)$ 是消息 $x$ 出现的概率，$l(x)$ 是消息 $x$ 对应码字的长度。

### 5. 术语定义

> *(教师板书)：*

**Message (消息/信源符号)：** 待编码的原始信息单元，来自集合 $\mathcal{X}$，如 $\{W, L, T\}$。
**Codeword (码字)：** 用编码字符集（如 $\{0, 1\}$）表示的消息。例如，将 $\{W, L, T\}$ 编码为 $\{c_W, c_L, c_T\}$。
**Code (码/码簿)：** 所有消息对应码字的集合 $C = \{c_W, c_L, c_T\}$。

---

## 三、唯一可译码 (Uniquely Decodable Codes) 与无前缀码 (Prefix-free Codes)

### 1. 无前缀码 (Prefix-free Codes) / 即时码 (Instantaneous Codes)：

**定义：** 在一个码集 $C$ 中，如果没有任何一个码字是另一个码字的前缀，则该码集称为无前缀码。
    - 中文常译为“无前缀码”而非“前缀码”，以避免歧义。

**编码方案2 (教师板书，一种 Prefix-free 编码)：**

> *(教师板书，一种 Prefix-free 编码)：*

$W \rightarrow 0$
$L \rightarrow 11$
$T \rightarrow 10$
此编码是无前缀码，因为 $0$ 不是 $11$ 或 $10$ 的前缀，$11$ 不是 $0$ 或 $10$ 的前缀，$10$ 也不是 $0$ 或 $11$ 的前缀。
**性质：** 无前缀码一定是唯一可译码。接收端接收到码流后，可以即时、无歧义地进行解码，无需等待后续比特。

### 2. 唯一可译码 (Uniquely Decodable Codes)：

**定义：** 如果任何由该码集中码字组成的序列，只能以一种方式分割回原始消息序列，则该码是唯一可译码。

### 3. 两类码集的关系

> *(教师板书)：*

令 $A$ 为所有无前缀码的集合， $B$ 为所有唯一可译码的集合。
则有：$A \subsetneq B$ (无前缀码的集合是唯一可译码集合的一个真子集)。
**解释：**
    - **1) Prefix-free is a sufficient condition (for unique decodability).** $\checkmark$
        所有无前缀码都是唯一可译的。
    - **2) Is prefix-free a necessary condition (for unique decodability)?** $\times$
        无前缀码不是唯一可译码的必要条件。存在一些码不是无前缀码，但仍然是唯一可译的。例如，对于信源符号 $\{s_1, s_2, s_3\}$，编码 $C = \{0, 01, 011\}$ 是唯一可译的，但不是无前缀码 (因为 $0$ 是 $01$ 的前缀，$01$ 是 $011$ 的前缀)。

### 4. 在最小化平均码长问题上的等价性：

由于 $A \subseteq B$，直观上，在更大的集合 $B$ 中寻找最小平均码长，其结果应小于或等于在子集 $A$ 中寻找的最小平均码长：
$$ \min_{C \in B} E[l(C)] \le \min_{C \in A} E[l(C)] $$
**关键结论 (麦克米兰定理的一个推论)：** 实际上这两个最小值是**相等**的：
$$ \min_{C \in B} E[l(C)] = \min_{C \in A} E[l(C)] $$
**推论与意义：**
    1. **存在性保证：** 对于任何一个在唯一可译码集合 $B$ 中达到最小平均码长的码 $C^* \in B$，必然**存在**一个无前缀码 $C^{**} \in A$，其平均码长与 $C^*$ 相同，即 $E[l(C^{**})] = E[l(C^*)]$。

> *(板书内容)*
            $$ \forall C^* \in B \text{ such that } E[l(C^*)] \text{ achieves the minimum} $$
            $$ \Rightarrow \exists C^{**} \in A, \text{ such that } E[l(C^{**})] = E[l(C^*)] $$

        2.  **简化搜索范围：** 这意味着，在寻求最小平均码长这一目标时，我们可以将搜索范围从更广泛、结构更复杂的唯一可译码集合 $B$ **限制到相对较小且结构更好的无前缀码集合 $A$**，而不会损失最优性。这是非常重要的，因为它大大简化了寻找最优码的问题（例如，哈夫曼编码就是在无前缀码中寻找最优码的算法）。

### 5. 课后思考题

> *(教师布置)：*

**问题1：** 构想一个例子，其中一个码是唯一可译的但不是无前缀的 ($C \in B, C \notin A$)，并且这个码确实达到了最小平均码长。
    - **解答思路与示例：**
        考虑信源 $\mathcal{X} = \{s_1, s_2, s_3\}$，其概率分布为 $P(s_1) = 1/2, P(s_2) = 1/4, P(s_3) = 1/4$。
        一个最优的无前缀码 (例如通过哈夫曼编码得到) 可以是 $C_{PF} = \{0, 10, 11\}$。其码长为 $l_1=1, l_2=2, l_3=2$。
        平均码长 $L_{avg}(C_{PF}) = (1/2) \cdot 1 + (1/4) \cdot 2 + (1/4) \cdot 2 = 0.5 + 0.5 + 0.5 = 1.5$ 比特/符号。
        现在考虑一个非无前缀码 $C_{nonPF} = \{0, 01, 11\}$。
        这个码是唯一可译的 (可以通过Sardinas-Patterson算法验证，或者直观观察，遇到 $11$ 必为 $s_3$，遇到 $0$ 后若为 $1$ 则为 $s_2$，否则为 $s_1$)。
        它不是无前缀码，因为 $0$ (码字 $s_1$) 是 $01$ (码字 $s_2$) 的前缀。
        其码长也是 $l_1=1, l_2=2, l_3=2$。
        平均码长 $L_{avg}(C_{nonPF}) = (1/2) \cdot 1 + (1/4) \cdot 2 + (1/4) \cdot 2 = 1.5$ 比特/符号。
        因此，$C_{nonPF} = \{0, 01, 11\}$ 是一个唯一可译但非无前缀的码，对于给定的概率分布，它也达到了最小平均码长。

**问题2：** 对于任意一个最优的唯一可译码 $C^* \in B$ (即 $E[l(C^*)]$ 最小)，如果它恰好不是无前缀码 ($C^* \notin A$)，思考是否能给出一个通用的方法或规则，将其调整为一个无前缀码 $C^{**} \in A$，同时保持其平均码长不变，即 $E[l(C^{**})] = E[l(C^*)]$。
    - **解答思路 (基于麦克米兰定理)：**
      麦克米兰定理 (McMillan's Theorem) 指出，一个码是唯一可译的，当且仅当其码长 $l_1, l_2, \dots, l_n$ 满足 Kraft 不等式： $\sum_{i=1}^{n} D^{-l_i} \le 1$ (对于 $D$ 元码，二进制时 $D=2$)。
      Kraft 不等式的另一部分（逆定理）则表明，如果一组码长 $l_1, l_2, \dots, l_n$ 满足该不等式，则必然存在一个具有这些码长的**无前缀码**。
      **构造性方法：**
      1.  给定一个最优的唯一可译码 $C^* \in B$ (非无前缀)，其码字长度分别为 $l_1^*, l_2^*, \dots, l_n^*$。
      2.  由于 $C^*$ 是唯一可译的，根据麦克米兰定理，其码长必须满足 Kraft 不等式： $\sum_{i=1}^{n} 2^{-l_i^*} \le 1$。
      3.  根据 Kraft 不等式的逆定理，既然码长序列 $\{l_i^*\}$ 满足该不等式，那么就一定可以构造出一个**无前缀码** $C^{**} \in A$，其码字长度恰好也是 $l_1^*, l_2^*, \dots, l_n^*$。
      4.  由于 $C^{**}$ 与 $C^*$ 具有完全相同的码长分布，且消息的概率分布 $P$ 相同，它们的平均码长必然相等：
          $$ E[l(C^{**})] = \sum p_i l_i^* = E[l(C^*)] $$
      因此，这个“调整”方法就是利用 Kraft 不等式的逆定理，根据最优唯一可译码的码长分布，重新构造一个具有相同码长分布的无前缀码。

---

## 四、Kraft 不等式 (Kraft Inequality)

既然研究最优编码可以聚焦于无前缀码，我们需要了解无前缀码的码长必须满足什么条件。

### 1. 定理 (Kraft Inequality for Prefix-free Codes):

假设有一个包含 $n$ 个信源符号的信源 $\mathcal{X} = \{x_1, \dots, x_n\}$，其对应的二进制无前缀码为 $C = (c_1, \dots, c_n)$，码字 $c_i$ 的长度为 $l_i$ (比特数)。那么，这些码长必须满足以下不等式：
$$ \sum_{i=1}^{n} 2^{-l_i} \le 1 $$

### 2. 定理的直观解释 (基于二叉树)：

任何一个二进制码字都可以看作是标准二叉树中的一条从根节点出发的路径 (例如，0 代表向左孩子，1 代表向右孩子)。
如果一个码是**无前缀码**，那么它的所有码字在对应的二叉树中都必须是**叶子节点 (leaves)**。因为如果一个码字 $c_j$ 是另一个码字 $c_k$ 的前缀，那么 $c_j$ 对应的节点就会是 $c_k$ 对应节点的祖先节点，这意味着 $c_j$ 不是叶子节点，与无前缀码的定义矛盾。

> *(教师板书示意图：一颗二叉树，其中叶子节点代表码字)*

**长度 $l_i$ 的含义：** 码字 $c_i$ 的长度 $l_i$ 对应于二叉树中该叶子节点的深度 (从根节点到该叶子节点的路径长度)。
**$2^{-l_i}$ 的含义：** 深度为 $l_i$ 的一个叶子节点可以看作“占据”了整个码空间 (所有可能的长度为 $l_i$ 的二进制序列形成的子空间) 的 $2^{-l_i}$ 的比例。例如，长度为1的码字 (如 '0') 占据了 $1/2$ 的空间；长度为2的码字 (如 '00') 占据了 $1/4$ 的空间。
**$\sum 2^{-l_i} \le 1$ 的含义：** 所有无前缀码字所占据的码空间比例之和不能超过1 (整个码空间)。
**等号成立条件：** 当且仅当这个无前缀码是“完备的”或“满的” (a full prefix code)，即二叉树中所有可用的分支都被利用，没有内部节点可以再向下生长出新的码字而不违反无前缀特性，且所有码字都是叶子节点时，等号成立，即 $\sum_{i=1}^{n} 2^{-l_i} = 1$。如果树不是“满”的（有些分支没有被利用到底，可以添加新的码字而不失其无前缀性），则和将严格小于1。

### 3. 结合目标：最小化平均码长

我们的目标是在所有无前缀码中，找到一组码长 $(l_1, \dots, l_n)$，使得对于给定的消息概率分布 $P=(p_1, \dots, p_n)$，平均码长 $\sum p_i l_i$ 最小。
优化问题可以表述为：

**目标 (Goal):**
寻找码长 $l_1, \dots, l_n$

最小化 (Minimize):
$$ L_{avg} = \sum_{i=1}^{n} p_i l_i $$

**约束条件 (Subject to / s.t.):**
1.  $\sum_{i=1}^{n} 2^{-l_i} \le 1$ (Kraft 不等式，保证存在对应的无前缀码)
2.  $l_i$ 是正整数 ($l_i \in \mathbb{Z}^+$)。

---

## 五、最优码长与熵 (Entropy) 的引出

### 1. 优化问题的数学松弛 (Relaxation)：码长 $l_i$ 视为实数

**实际情况：** 在实际编码中，码字的长度 $l_i$ (比特数) 必然是**正整数**。
**数学上的放松：** 为了使用连续优化的方法 (如拉格朗日乘子法) 求解上述优化问题，暂时**不考虑** $l_i$ 必须是整数的约束，即允许 $l_i$ 为**正实数** ($l_i > 0, l_i \in \mathbb{R}^+$)。

> *(教师在板书 $l_i > 0$ 下方一度写了 $l_i \in \mathbb{N}$，但随后擦除，强调暂时不考虑整数约束)*

**意义：** 这样得到的最小平均码长，将是真实情况下 ($l_i$ 为整数时) 的最小平均码长的一个**下界 (lower bound)**。

2.  **Kraft 不等式约束的进一步讨论与强化：**
    - 学生提问：如果 Kraft 不等式是 $\sum 2^{-l_i} < 1$ (严格小于1)，这是否还能完全反映无前缀码的特性？
    - 教授解答：Kraft 不等式的原始形式是 $\sum 2^{-l_i} \le 1$。如果和严格小于1，意味着码对应的二叉树不是“满”的，但它仍然是一个有效的无前缀码。

### 2. Kraft 不等式约束的进一步讨论与强化：

**强化为等式：** 在求解最小平均码长 $\min \sum p_i l_i$ 的优化问题时 (已放松 $l_i$ 为实数)，我们可以将约束 $\sum_{i=1}^{n} 2^{-l_i} \le 1$ **替换为** $\sum_{i=1}^{n} 2^{-l_i} = 1$ 而不损失最优性。
- 直观解释：假设最优解 $\{l_i^*\}$ 使得 $\sum 2^{-l_i^*} < 1$。由于目标是最小化 $\sum p_i l_i$ (其中 $p_i > 0$)，这意味着我们希望 $l_i$ 尽可能小。如果 $\sum 2^{-l_i^*} < 1$，那么总存在至少一个 $l_j^*$ 可以被适当减小一点点 (比如变成 $l_j^* - \epsilon$)，使得新的码长集合 $\{l_i'\}$ 仍然满足 $\sum 2^{-l_i'} \le 1$ (因为 $2^{-x}$ 是 $x$ 的减函数，减小 $l_j^*$ 会增大 $2^{-l_j^*}$，从而使和更接近1)，但平均码长 $\sum p_i l_i'$ 会因此变得更小。这与原解是最优的假设矛盾。因此，最优解必然使 Kraft 不等式取等号。

> *(教师在板书 $\sum_{i=1}^{n} 2^{-l_i} \le 1$ 的 "$\le$" 下方加了一横线，变成 "$\underline{\underline \le}$"，然后擦掉 "$\le$" 写上 "$=$"。) *

### 3. 求解松弛后的优化问题：

**优化问题 (已松弛 $l_i$ 为实数，并强化 Kraft 不等式为等式):**
$$ \min_{l_1, \dots, l_n} \sum_{i=1}^{n} p_i l_i $$

**约束 (s.t.):**
1.  $\sum_{i=1}^{n} 2^{-l_i} = 1$
2.  $l_i > 0$ (且为实数)

**求解方法：** 该问题可以使用**拉格朗日乘子法 (Lagrange Multipliers)** 解决。
构造拉格朗日函数:
$$ \mathcal{L}(l_1, \dots, l_n, \lambda) = \sum_{i=1}^{n} p_i l_i + \lambda \left( \sum_{i=1}^{n} 2^{-l_i} - 1 \right) $$
对每个 $l_k$ 求偏导并令其为0：
$$ \frac{\partial \mathcal{L}}{\partial l_k} = p_k + \lambda \cdot 2^{-l_k} \ln(2) (-1) = p_k - \lambda \ln(2) 2^{-l_k} = 0 $$
$$ p_k = \lambda \ln(2) 2^{-l_k} $$
因此，$2^{-l_k} = \frac{p_k}{\lambda \ln(2)}$。
代入约束 $\sum 2^{-l_i} = 1$:
$$ \sum_{i=1}^{n} \frac{p_i}{\lambda \ln(2)} = 1 \Rightarrow \frac{1}{\lambda \ln(2)} \sum_{i=1}^{n} p_i = 1 $$
由于 $\sum p_i = 1$ (概率之和为1)，所以 $\frac{1}{\lambda \ln(2)} = 1 \Rightarrow \lambda = \frac{1}{\ln(2)}$.
将 $\lambda$ 代回 $p_k = \lambda \ln(2) 2^{-l_k}$：
$$ p_k = \frac{1}{\ln(2)} \ln(2) 2^{-l_k} = 2^{-l_k} $$
    *   **最优理想码长 (当 $l_i$ 可为实数时):**
        从 $p_i = 2^{-l_i^*}$，两边取以2为底的对数，得到：
        $$ l_i^* = \log_2 \frac{1}{p_i} = -\log_2 p_i $$
        (这里要求 $p_i > 0$。若 $p_i=0$，则该符号永不出现，其码长可视为无穷或不分配码字)。

### 4. 熵 (Entropy) 的定义 (Shannon Entropy)：

将此最优理想码长 $l_i^* = \log_2 \frac{1}{p_i}$ 代入平均码长公式 $\sum p_i l_i$，得到此时的理论最小平均码长：
$$ L_{ideal\_min} = \sum_{i=1}^{n} p_i l_i^* = \sum_{i=1}^{n} p_i \log_2 \frac{1}{p_i} $$
这个量被香农定义为离散随机信源 $X$ (其取值遵循概率分布 $P=(p_1, \dots, p_n)$) 的**熵 (Entropy)**，记作 $H(X)$ 或 $H(P)$：
$$ H(X) := \sum_{i=1}^{n} p_i \log_2 \frac{1}{p_i} $$
也可以写成：
$$ H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i $$
(约定 $0 \log_2 0 = 0$，因为 $\lim_{p \to 0^+} p \log_2 p = 0$)

---

## 六、熵的性质与意义

### 1. 理论最小平均码长 (Minimum Average Code Length):

熵 $H(X)$ 代表了在**允许码长为非整数**的理想情况下，对随机信源 $X$ 进行无前缀编码所能达到的**理论最小平均码长**。
**香农第一定理 (无噪信源编码定理) 的核心思想：** 实际编码中 $l_i$ 必须是整数。可以证明，对于任何信源 $X$，存在一种无前缀码，其平均码长 $L_{avg}$ 满足：
$$ H(X) \le L_{avg} < H(X) + 1 $$
这意味着，实际最优编码 (如哈夫曼编码) 的平均码长 $L_{min}$ 最多只比熵值大不到 1 比特。当对信源的扩展进行编码 (即一次编码多个符号) 时，每符号的平均码长可以任意逼近熵。因此，熵是衡量数据可压缩性的一个基本下限。

### 2. 信息量的量化 (Quantification of Information):

熵 $H(X)$ 是衡量一个随机信源（或随机变量 $X$）平均含有多少**信息量**的一种方式。单位通常是比特/符号 (bits per symbol).

### 3. 不确定性的度量 (Measure of Uncertainty):

熵也是对随机信源输出结果不确定性的一种度量。
**均匀分布 (Uniform distribution):** 当所有 $n$ 个消息等概率出现时 ($p_i = 1/n$ for all $i$)，信源的不确定性最大，此时熵 $H(X)$ 达到其最大值：
$$ H(X) = \sum_{i=1}^n \frac{1}{n} \log_2 n = n \cdot \frac{1}{n} \log_2 n = \log_2 n $$

> *(教师板书示意图：画了一个二元熵函数 $H_b(p) = -p\log_2 p - (1-p)\log_2(1-p)$ 的图像，在 $p=1/2$ 时，不确定性最大，熵达到最大值 $H_b(1/2) = 1$ bit。)*

**确定性情况 (Deterministic):** 如果某个消息 $x_k$ 以概率1出现 ($p_k=1$)，其他消息概率为0，那么信源是完全确定的，不确定性为0，此时熵 $H(X)=0$ (因为 $1 \log_2 1 = 0$ 且 $0 \log_2 0 = 0$)。

### 4. 平均信息内容 (Average Information Content):

**自信息 (Self-information) / 信息内容 (Information content)：**
可以将 $I(x_i) = \log_2 \frac{1}{p_i} = -\log_2 p_i$ 视为当消息 $x_i$ (或事件 $X=x_i$) 发生时，它所携带的**信息内容**或**自信息**。
- $p_i$ 越小 (事件越稀有、越出乎意料)，$\log_2 \frac{1}{p_i}$ 越大，表示该消息携带的信息量越大，编码它所需要的最优（理想）码长也越长。
- $p_i$ 越大 (事件越常见、越确定)，$\log_2 \frac{1}{p_i}$ 越小，信息量越小，最优码长也越短。
- 如果 $p_i = 1$ (确定事件)，则 $I(x_i) = \log_2 \frac{1}{1} = 0$，不携带任何新信息。
熵 $H(X) = \sum_{i=1}^{n} p_i I(x_i) = E[I(X)]$ 就是这些单个消息自信息的**期望值 (expected value)**，即平均每个信源符号所携带的信息量。

---

## 七、本节总结与展望

本节课主要回顾了信息论的基本研究范畴，并深入探讨了信息量化与编码的核心概念。通过足球比赛结果的编码示例，阐释了唯一可译性和无前缀码的重要性，并证明了在寻求最小平均码长时，可以仅限于无前缀码。

关键步骤包括：
1.  引入 **Kraft 不等式** $\sum 2^{-l_i} \le 1$ 作为无前缀码码长必须满足的条件。
2.  在寻求最小平均码长 $\sum p_i l_i$ 的优化问题中，通过放松码长 $l_i$ 为实数，并将 Kraft 不等式约束强化为等式 $\sum 2^{-l_i} = 1$。
3.  求解该优化问题得到理想最优码长为 $l_i^* = \log_2 \frac{1}{p_i}$。
4.  基于此，定义了信源的**熵 $H(X) = \sum p_i \log_2 \frac{1}{p_i}$**。

熵 $H(X)$ 不仅是理论上的最小平均码长（允许码长为非整数），也是对信源所含平均信息量和不确定性的度量。单个事件 $x_i$ 的信息内容（自信息）为 $\log_2 \frac{1}{p_i}$。

这些概念为后续学习更高级的信源编码定理（如哈夫曼编码的性能分析）和信道编码定理奠定了坚实的基础。
